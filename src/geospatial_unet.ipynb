{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae7601f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Geospatial Analysis with Machine Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85955005",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Imports & Environment Settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bdc72",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports\n",
    "\n",
    "<!-- _You may see a message about regarding the use of a Tensorflow binary that is optimized with oneAPI Deep Neural Network Library (oneDNN). There is nothing wrong and it can be safely ignored._ -->\n",
    "\n",
    "Run the following cell to install and import neccessary libraries for this workflow. A temporary directory is created to store the data and model files.\n",
    "\n",
    "> Please note that if you rerun this cell, the temporary directory will be deleted and recreated. If you want to keep the data and model files, please copy them to a permanent location before rerunning this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries & create a temporary working directory in current folder\n",
    "# !pip install --quiet contextily earthpy fiona geopandas rasterio pyproj keras-spatial spectral\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import alphashape\n",
    "import contextily as cx\n",
    "import cv2\n",
    "import earthpy.spatial as es\n",
    "import fiona as fio\n",
    "import geopandas as gpd\n",
    "import imgaug as ia\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from osgeo import gdal\n",
    "import rasterio\n",
    "import rasterio as rio\n",
    "import spectral\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from imgaug import augmenters as iaa\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras_spatial import SpatialDataGenerator\n",
    "from rasterio import windows\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.warp import Resampling, calculate_default_transform, reproject\n",
    "from shapely.geometry import box\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = time.time()\n",
    "\n",
    "# load functions and set global variables\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data dir for temporary/working files in the current directory\n",
    "working_dir = os.path.join(\".\", \"working\")\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "print(\n",
    "    f\"{time.ctime()}: Created a temporary working directory in current folder at {working_dir}\"\n",
    ")\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "\n",
    "# helper functions & global variables for the workflow\n",
    "PIXEL_SIZE = -1\n",
    "TILE_SIZE = -1\n",
    "\n",
    "\n",
    "def reproject_raster(in_path, out_path, to_crs):\n",
    "    # reproject raster to project crs\n",
    "    with rio.open(in_path) as src:\n",
    "        if src.crs == to_crs:\n",
    "            print(f\"{time.ctime()}: {in_path} is already in target CRS.\")\n",
    "            return in_path\n",
    "\n",
    "        src_crs = src.crs\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src_crs, to_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        kwargs = src.meta.copy()\n",
    "\n",
    "        kwargs.update(\n",
    "            {\"crs\": to_crs, \"transform\": transform, \"width\": width, \"height\": height}\n",
    "        )\n",
    "\n",
    "        with rio.open(out_path, \"w\", **kwargs) as dst:\n",
    "            for i in tqdm(range(1, src.count + 1)):\n",
    "                reproject(\n",
    "                    source=rio.band(src, i),\n",
    "                    destination=rio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=to_crs,\n",
    "                    resampling=Resampling.nearest,\n",
    "                )\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def encode(data_path, feature):\n",
    "    # create a numeric unique value for each attribute/feature in the data feature\n",
    "    # vector = data.copy()\n",
    "\n",
    "    data = gpd.read_file(data_path)\n",
    "    data.columns = map(str.lower, data.columns)\n",
    "\n",
    "    data = data.dropna(subset=[\"geometry\"])\n",
    "\n",
    "    feature = feature.lower()\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data[feature])\n",
    "\n",
    "    data[f\"{feature}_encoded\"] = le.transform(data[feature])\n",
    "    data[f\"{feature}_encoded\"] += 1\n",
    "\n",
    "    data.to_file(data_path)\n",
    "\n",
    "\n",
    "def get_windows(window_shape, image_shape):\n",
    "    win_rows, win_cols = window_shape\n",
    "    img_rows, img_cols = image_shape\n",
    "    offsets = itertools.product(\n",
    "        range(0, img_cols, win_cols), range(0, img_rows, win_rows)\n",
    "    )\n",
    "    image_window = windows.Window(col_off=0, row_off=0, width=img_cols, height=img_rows)\n",
    "\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(\n",
    "            col_off=col_off, row_off=row_off, width=win_cols, height=win_rows\n",
    "        )\n",
    "\n",
    "        yield window.intersection(image_window)\n",
    "\n",
    "\n",
    "# def to_raster(\n",
    "#     data_path,\n",
    "#     output_path,\n",
    "#     feature_id,\n",
    "#     pixel_size=PIXEL_SIZE,\n",
    "#     dtype=\"float64\",\n",
    "#     windows_shape=(1024, 1024),\n",
    "# ):\n",
    "#     encode(data_path, feature_id)\n",
    "\n",
    "#     with fio.open(data_path) as features:\n",
    "#         crs = features.crs\n",
    "#         xmin, ymin, xmax, ymax = features.bounds\n",
    "#         transform = rio.Affine.from_gdal(xmin, pixel_size, 0, ymax, 0, -pixel_size)\n",
    "#         out_shape = (int((ymax - ymin) / pixel_size), int((xmax - xmin) / pixel_size))\n",
    "\n",
    "#         with rio.Env(CHECK_DISK_FREE_SPACE=\"NO\"):\n",
    "\n",
    "#             with rio.open(\n",
    "#                 output_path,\n",
    "#                 \"w\",\n",
    "#                 height=out_shape[0],\n",
    "#                 width=out_shape[1],\n",
    "#                 count=1,\n",
    "#                 dtype=dtype,\n",
    "#                 crs=crs,\n",
    "#                 transform=transform,\n",
    "#                 tiled=True,\n",
    "#                 options=[\"COMPRESS=LZW\"],\n",
    "#             ) as raster:\n",
    "#                 for window in get_windows(windows_shape, out_shape):\n",
    "#                     window_transform = windows.transform(window, transform)\n",
    "#                     # can be smaller than windows_shape at the edges\n",
    "#                     window_shape = (window.height, window.width)\n",
    "#                     window_data = np.zeros(window_shape)\n",
    "\n",
    "#                     for feature in features:\n",
    "#                         value = feature[\"properties\"][f\"{feature_id}_encoded\"]\n",
    "#                         geom = feature[\"geometry\"]\n",
    "#                         d = rasterize(\n",
    "#                             [(geom, value)],\n",
    "#                             all_touched=False,\n",
    "#                             out_shape=window_shape,\n",
    "#                             transform=window_transform,\n",
    "#                         )\n",
    "#                         window_data += d  # sum values up\n",
    "\n",
    "#                     raster.write(window_data, window=window, indexes=1)\n",
    "def to_raster(\n",
    "    data_path,\n",
    "    output_path,\n",
    "    feature_id,\n",
    "    pixel_size=PIXEL_SIZE,\n",
    "    dtype=\"float64\",\n",
    "    windows_shape=(1024, 1024),\n",
    "):\n",
    "    encode(data_path, feature_id)\n",
    "\n",
    "    with fio.open(data_path) as features:\n",
    "        crs = features.crs\n",
    "        xmin, ymin, xmax, ymax = features.bounds\n",
    "        transform = rio.Affine.from_gdal(xmin, pixel_size, 0, ymax, 0, -pixel_size)\n",
    "        out_shape = (int((ymax - ymin) / pixel_size), int((xmax - xmin) / pixel_size))\n",
    "\n",
    "        with rio.open(\n",
    "            output_path,\n",
    "            \"w\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=dtype,\n",
    "            crs=crs,\n",
    "            transform=transform,\n",
    "            tiled=True,\n",
    "            options=[\"COMPRESS=LZW\"],\n",
    "        ) as raster:\n",
    "            for window in get_windows(windows_shape, out_shape):\n",
    "                window_transform = windows.transform(window, transform)\n",
    "                # can be smaller than windows_shape at the edges\n",
    "                window_shape = (window.height, window.width)\n",
    "                window_data = np.zeros(window_shape)\n",
    "\n",
    "                for feature in features:\n",
    "                    value = feature[\"properties\"][f\"{feature_id}_encoded\"]\n",
    "                    geom = feature[\"geometry\"]\n",
    "                    d = rasterize(\n",
    "                        [(geom, value)],\n",
    "                        all_touched=False,\n",
    "                        out_shape=window_shape,\n",
    "                        transform=window_transform,\n",
    "                    )\n",
    "                    window_data += d  # sum values up\n",
    "\n",
    "                raster.write(window_data, window=window, indexes=1)\n",
    "\n",
    "def closestDivisibleNumber(n, m):\n",
    "    if n % m == 0:\n",
    "        return n\n",
    "\n",
    "    return n - (n % m)\n",
    "\n",
    "\n",
    "def crop_center(img, cropx, cropy):\n",
    "    threeD = False\n",
    "    try:\n",
    "        _, x, y = img.shape\n",
    "        threeD = True\n",
    "    except:\n",
    "        x, y = img.shape\n",
    "\n",
    "    startx = x // 2 - cropx // 2\n",
    "    starty = y // 2 - cropy // 2\n",
    "\n",
    "    if threeD:\n",
    "        return img[:, startx : startx + cropx, starty : starty + cropy]\n",
    "\n",
    "    return img[startx : startx + cropx, starty : starty + cropy]\n",
    "\n",
    "\n",
    "def concave_hull(dataframe, degree=0.001):\n",
    "    \"\"\"Create a single concave hull of an input GeoPandas DataFrame\"\"\"\n",
    "    flat_list = []\n",
    "\n",
    "    # Iterate over each geometry in the DataFrame\n",
    "    for geom in dataframe[\"geometry\"]:\n",
    "        # Check if the geometry is a MultiPolygon\n",
    "        if geom.geom_type == \"MultiPolygon\":\n",
    "            # Iterate over each polygon within the MultiPolygon\n",
    "            for polygon in geom.geoms:\n",
    "                # Extract the exterior coordinates of the polygon\n",
    "                flat_list.extend(list(polygon.exterior.coords))\n",
    "        else:\n",
    "            # Extract the exterior coordinates of the geometry\n",
    "            flat_list.extend(list(geom.exterior.coords))\n",
    "\n",
    "    # Create the concave hull\n",
    "    vertices = [(x, y) for x, y in flat_list]\n",
    "    # alpha = alphashape.optimizealpha(vertices) / 2\n",
    "    hull = alphashape.alphashape(vertices, degree)\n",
    "\n",
    "    # Create a GeoDataFrame with the concave hull\n",
    "    result = gpd.GeoDataFrame(geometry=[hull], crs=dataframe.crs)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0cfc2",
   "metadata": {},
   "source": [
    "## Setting a random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03380f96",
   "metadata": {},
   "source": [
    "### What is a random seed?\n",
    "\n",
    "A random seed is a number that is used to initialize a pseudorandom number generator. This is used to generate a sequence of numbers that are seemingly random, but are actually deterministic. This is useful for reproducibility, as the same seed will always generate the same sequence of \"random\" numbers.\n",
    "\n",
    "In short, this allows the results of this workflow to be reproducible.\n",
    "\n",
    "Set the random seed to any integer value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ed913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random seed for reproducibility\n",
    "seed_picker = widgets.IntText(value=42, description=\"Seed:\", disabled=False)\n",
    "\n",
    "seed_picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "SEED = seed_picker.value\n",
    "\n",
    "# set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(SEED)\n",
    "\n",
    "# set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# configure a new global `tensorflow` session\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1\n",
    ")\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031f898",
   "metadata": {},
   "source": [
    "# Add Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e1f81",
   "metadata": {},
   "source": [
    "## Upload your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b4c31",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Features are the inputs the model learns in order to predict a _mask_. For example, if you want to predict the land cover of a region, a feature may be soil type.\n",
    "\n",
    "Make sure to input all of the files you want to upload at once.\n",
    "\n",
    "> If you plan to re-run this workflow with the same data, you can expand the cell and and the paths to the `value` parameter, which sets a default value. For example, `value = '...data/feature1.tif,...data/feature2.tif'`.\n",
    "\n",
    "### Mask\n",
    "\n",
    "A mask defines the output the model learns to predict. For example, if you want to predict the land cover of a region, the masks would be polygons representing the land cover types.\n",
    "\n",
    "As of right now, this workflow only supports the ability to predict a single mask.\n",
    "\n",
    "You can only upload a single **vector** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the paths to your data files\n",
    "\n",
    "add_features = widgets.Textarea(\n",
    "    value=\"../data/chile_data/2328825_2011-08-13_RE1_3A_Analytic.tif\", #../data/chile_data/alos_nova_friburgo_5m.tif,\n",
    "    # value=\"../data/california_data/dem.tif,../data/california_data/tri.tif,../data/california_data/slope.tif,../data/california_data/roughness.tif\",\n",
    "    # value=\"../data/drumlin_data/lidar.tif\",\n",
    "    placeholder=\"File paths (comma-separated)\",\n",
    "    description=\"Features:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "add_mask = widgets.Textarea(\n",
    "    value=\"../data/chile_data/scars.geojson\",\n",
    "    # value='../data/califo/rnia_data/landslide_deposits.gpkg',\n",
    "    # value=\"../data/drumlin_data/mask_rasterized.tif\",\n",
    "    placeholder=\"File path\",\n",
    "    description=\"Mask:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = [add_features, add_mask]\n",
    "tabs.set_title(0, \"Input Feature Paths\")\n",
    "tabs.set_title(1, \"Input Mask Path\")\n",
    "\n",
    "tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91258aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the input data files to the working directory\n",
    "mask_path = shutil.copy2(\n",
    "    add_mask.value, os.path.join(working_dir, pathlib.Path(add_mask.value).name)\n",
    ")\n",
    "print(f\"{time.ctime()}: Saved a working copy of the mask to {mask_path}.\")\n",
    "\n",
    "feature_paths = []\n",
    "for f in add_features.value.split(\",\"):\n",
    "    path = os.path.join(working_dir, pathlib.Path(f).name)\n",
    "    print(f\"{time.ctime()}: Saved a working copy of the feature to {path}.\")\n",
    "    feature_paths.append(path)\n",
    "    try:\n",
    "        shutil.copy2(f, path)\n",
    "    except:\n",
    "        pass  # ignore if same file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb5d45",
   "metadata": {},
   "source": [
    "## Set the No Data Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd64fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the data has a nodata value\n",
    "# default to the mask nodata value if set\n",
    "# otherwise the first feature nodata value\n",
    "# otherwise set to 0\n",
    "nodata = 0\n",
    "\n",
    "if mask_path.endswith(\".tif\"):\n",
    "    for path in [mask_path, *feature_paths]:\n",
    "        with rio.open(path) as src:\n",
    "            try:\n",
    "                nodata = src.nodata\n",
    "            except:\n",
    "                nodata = 0\n",
    "\n",
    "            print(f\"{time.ctime()}: {path} has a nodata value of {nodata}.\")\n",
    "\n",
    "            if path == mask_path:\n",
    "                mask_nodata = nodata\n",
    "            else:\n",
    "                feature_nodata = nodata\n",
    "\n",
    "select_nodata = widgets.IntText(value=nodata, description=\"No-data:\", disabled=False)\n",
    "\n",
    "select_nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed4812",
   "metadata": {},
   "source": [
    "## Select a Coordinate Reference System (CRS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d70efd",
   "metadata": {},
   "source": [
    "Input data does not all need to have the same CRS, it will be reprojected to the CRS selected here. The CRS of your mask input will be set to the default value.\n",
    "\n",
    "This workflow can use any CRS accepted by the function [`pyproj.CRS.from_user_input()`](https://geopandas.org/en/stable/docs/user_guide/projections.html):\n",
    "\n",
    "- CRS WKT string\n",
    "- An authority string (i.e. \"EPSG:4326\")\n",
    "- An EPSG integer code (i.e. 4326)\n",
    "- A pyproj.CRS\n",
    "- An object with a to_wkt method\n",
    "- PROJ string\n",
    "- Dictionary of PROJ parameters\n",
    "- PROJ keyword arguments for parameters\n",
    "- JSON string with PROJ parameters\n",
    "\n",
    "\n",
    "For reference, some common projections and their codes:\n",
    "\n",
    "- WGS84 Latitude/Longitude: \"EPSG:4326\"\n",
    "- UTM Zones (North): \"EPSG:32633\"\n",
    "- UTM Zones (South): \"EPSG:32733\"\n",
    "\n",
    "<!-- TODO add details about what is a CRS (geographic vs projected), which to select, details about why there are limited options, etc -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b47da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a CRS\n",
    "NO_DATA = select_nodata.value\n",
    "\n",
    "# set a base CRS\n",
    "default = \"EPSG:4326\"\n",
    "\n",
    "# find the CRS from the mask file\n",
    "try:  # to open the mask file as a polygon\n",
    "    tmp = gpd.read_file(mask_path)\n",
    "    default = tmp.crs\n",
    "except:  # try to open the mask file as a raster\n",
    "    tmp = rio.open(mask_path)\n",
    "    default = tmp.crs\n",
    "\n",
    "print(f\"{time.ctime()}: Detected CRS from {mask_path} to be {default}\")\n",
    "\n",
    "select_crs = widgets.Text(\n",
    "    value=str(default).upper(), description=\"CRS:\", disabled=False\n",
    ")\n",
    "\n",
    "select_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609b4a0",
   "metadata": {},
   "source": [
    "## Reproject Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2be5f",
   "metadata": {},
   "source": [
    "If *necessary*, each of the input features and the mask will be reprojected to the CRS selected above.\n",
    "\n",
    "If the following cell fails to run, try rerunning the cell. If you are still having issues, you may be using an incompatible CRS for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data to the same CRS\n",
    "CRS = select_crs.value  # get the selected CRS from the widget\n",
    "\n",
    "min_res = -1  # set to negative 1\n",
    "max_res = np.inf  # set to infinity\n",
    "\n",
    "# reproject all data to the same CRS\n",
    "for data in [mask_path, *feature_paths]:\n",
    "    try:\n",
    "        reproject_raster(data, data, CRS)\n",
    "        # calculate the minimum resolution (worst resolution of the data)\n",
    "        min_res = max(min_res, rio.open(data).res[0])\n",
    "        # calculate the maximum resolution (best resolution of the data)\n",
    "        max_res = min(max_res, rio.open(data).res[0])\n",
    "    except:\n",
    "        gdf = gpd.read_file(data)\n",
    "        if gdf.crs == CRS:\n",
    "            print(f\"{time.ctime()}: {data} is already in target CRS.\")\n",
    "        else:\n",
    "            gdf.to_crs(CRS, inplace=True)\n",
    "            os.remove(data)\n",
    "            gdf.to_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67329c",
   "metadata": {},
   "source": [
    "## Determine the bounds of the area of interest\n",
    "\n",
    "<!-- Bounds define a rectangular area of interest.\n",
    "\n",
    "You may input the bounds manually or it will be assumed that the bounds are equivalent to the extent of the mask data. -->\n",
    "\n",
    "<!-- ### Upload a file describing the bound -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e31a35",
   "metadata": {},
   "source": [
    "The bounds of the area of interest can be determined in three ways:\n",
    "\n",
    "1. DEFAULT: The bounds will be determined by a **concave hull polygon** of the mask data. Note that only works if your mask data is provided in the form of a vector file.\n",
    "2. Automatic determination of the **total bounds** (rectangle that encompasses all mask polygons). For more information, please see the [documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.total_bounds.html).\n",
    "3. Upload a custom area definition in the form of a vector file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd05a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select bounds determination method\n",
    "\n",
    "options = [\"Concave Hull (default)\", \"Convex Hull\", \"Total Bounds\", \"Custom Bounds\"]\n",
    "select_bounds_method = widgets.Dropdown(\n",
    "    value=options[0], description=\"Method:\", options=options, disabled=False\n",
    ")\n",
    "\n",
    "select_bounds_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475960d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the bounds\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape\n",
    "\n",
    "if select_bounds_method.value == options[3]:\n",
    "    # create a widget to select custom bounds\n",
    "    upload_bounds = widgets.Text( # todo remove default value\n",
    "        value=\"../data/drumlin_data/bounds.geojson\", description=\"Bounds:\", disabled=False,\n",
    "    )\n",
    "\n",
    "    # display the widget\n",
    "    display(upload_bounds)\n",
    "\n",
    "else:\n",
    "    mask_polygons = gpd.read_file(mask_path)\n",
    "\n",
    "    if select_bounds_method.value == options[0]:\n",
    "        # concave hull of the mask region\n",
    "        # todo: make the degree of concavity a widget (lower is looser)\n",
    "        bounds_gpd = concave_hull(mask_polygons, 0.0001)\n",
    "\n",
    "        # rectangular max bounds of the mask region (for cropping)\n",
    "        total_bounds = mask_polygons.total_bounds\n",
    "        total_bounds_poly = box(*total_bounds)\n",
    "        total_bounds_gs = gpd.GeoSeries(total_bounds_poly, crs=CRS)\n",
    "\n",
    "    elif select_bounds_method.value == options[1]:\n",
    "        # convex hull of the mask region\n",
    "        convex_hull = mask_polygons[mask_polygons.is_valid].unary_union.convex_hull\n",
    "        bounds_gpd = gpd.GeoDataFrame(geometry=[convex_hull], crs=CRS)\n",
    "\n",
    "        # rectangular max bounds of the mask region (for cropping)\n",
    "        total_bounds = mask_polygons.total_bounds\n",
    "        total_bounds_poly = box(*total_bounds)\n",
    "        total_bounds_gs = gpd.GeoSeries(total_bounds_poly, crs=CRS)\n",
    "\n",
    "    elif select_bounds_method.value == options[2]:\n",
    "        total_bounds = mask_polygons.total_bounds\n",
    "        total_bounds_poly = box(*total_bounds)\n",
    "        total_bounds_gs = gpd.GeoSeries(total_bounds_poly, crs=CRS)\n",
    "        bounds_gpd = total_bounds_gs\n",
    "    else:\n",
    "        # throw an error\n",
    "        raise Exception(\"Custom bounds not yet implemented for rasters.\")\n",
    "\n",
    "# save the bounds to a file\n",
    "bounds_path = os.path.join(working_dir, \"bounds.geojson\")\n",
    "bounds_gpd.to_file(bounds_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe64c71",
   "metadata": {},
   "source": [
    "### Visualize the bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18ab2b",
   "metadata": {},
   "source": [
    "The bounds will be plotted on a basemap (if available) for reference. Specifically, Esri's National Geographic World Map will be used.\n",
    "\n",
    "- The bounds will be plotted as a blue polygon.\n",
    "- The masks will be plotted in red for reference.\n",
    "\n",
    "A GEOJSON file describing the determined bounds will be saved to the working directory. This file can be used to visualize the bounds in a GIS software such as QGIS or ArcGIS.\n",
    "\n",
    "> If your area of interest is small, the background reference map may not load--this does not affect the workflow, the basemap is only provided at this step for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the bounds\n",
    "\n",
    "\n",
    "# custom bounds\n",
    "if select_bounds_method.value == options[2]:\n",
    "    # read bounds from file\n",
    "    bounds_gpd = gpd.read_file(upload_bounds.value)\n",
    "\n",
    "    # reproject bounds to the CRS\n",
    "    bounds_gpd = bounds_gpd.to_crs(CRS)\n",
    "\n",
    "    # get the total bounds for cropping\n",
    "    total_bounds = bounds_gpd.total_bounds\n",
    "    total_bounds_poly = box(*total_bounds)\n",
    "\n",
    "    # save the bounds poly to file\n",
    "    bounds_gpd = gpd.GeoDataFrame(geometry=[total_bounds_poly], crs=CRS)\n",
    "    bounds_gpd.to_file(os.path.join(working_dir, \"bounds.geojson\"))\n",
    "\n",
    "    # # save the bounds to file\n",
    "    # bounds_gpd.to_file(os.path.join(working_dir, \"bounds.geojson\"))\n",
    "    # print(f'{time.ctime()}: Saved bounds to {os.path.join(working_dir, \"bounds.geojson\")}')\n",
    "elif select_bounds_method.value == options[3]:\n",
    "    try:\n",
    "        # get the bounds from the widget\n",
    "        bounds_gpd = gpd.read_file(upload_bounds.value)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# plot the bounds\n",
    "bounds_ax = bounds_gpd.boundary.plot(figsize=(8, 8))\n",
    "\n",
    "# add a title to the plot\n",
    "bounds_ax.set_title(\"Bounds\")\n",
    "\n",
    "# add axis labels\n",
    "bounds_ax.set_xlabel(\"Longitude\")\n",
    "bounds_ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# add a basemap to the plot\n",
    "try:\n",
    "    cx.add_basemap(bounds_ax, source=cx.providers.Esri.WorldTopoMap, crs=CRS)\n",
    "except:\n",
    "    print(f\"{time.ctime()}: Failed to add basemap to the plot.\")\n",
    "\n",
    "try:\n",
    "    # add the masks to the plot\n",
    "    mask_polygons.plot(ax=bounds_ax, color=\"red\")\n",
    "except:\n",
    "    # add the raster to the plot\n",
    "    with rio.open(mask_path) as src:\n",
    "        show(src, ax=bounds_ax, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c16c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area = bounds_gpd.area[0]\n",
    "unit = bounds_gpd.crs.axis_info[0].unit_name\n",
    "print(\n",
    "    f\"{time.ctime()}: Total area of the bounds is {total_area:.2f} in square {unit}s.\"\n",
    ")\n",
    "\n",
    "# calculate the area of the masks\n",
    "try:\n",
    "    obj_area = mask_polygons.area.sum()\n",
    "except:\n",
    "    with rio.open(mask_path) as src:\n",
    "        # convert to numpy\n",
    "        mask = src.read(1)\n",
    "\n",
    "        # sum where mask is 1\n",
    "        obj_area = (mask == 1).sum() * src.res[0] ** 2\n",
    "\n",
    "print(f\"{time.ctime()}: Total area of the masks is {obj_area:.2f} square {unit}s.\")\n",
    "\n",
    "ratio = obj_area / total_area\n",
    "print(f\"{time.ctime()}: Ratio of masks to bounds is {ratio:.3f}.\")\n",
    "\n",
    "CLASS_WEIGHTS = {0: 1 - ratio, 1: ratio}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823b355",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8dca3",
   "metadata": {},
   "source": [
    "## Select tile size & data resolution\n",
    "\n",
    "<!-- ### What is a tile size?\n",
    "\n",
    "### Why are tile sizes powers of 2?\n",
    "\n",
    "### How to choose a tile size -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84aa11",
   "metadata": {},
   "source": [
    "The tile size is the size of the image patches in `tile_size * tile_size` pixels that will be extracted from the input data.\n",
    "\n",
    "The resolution is automatically determined by the input data, but can be modified.\n",
    "\n",
    "<!-- It is not recommended increase the resolution of the data -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tile size and data resolution\n",
    "select_tile_size = widgets.Dropdown(\n",
    "    options=[16, 32, 64, 128, 256, 512],\n",
    "    value=64,\n",
    "    description=\"Tile Size:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# set the minimum resolution to be the maximum of the minimum resolution and the maximum resolution\n",
    "min_res = max(min_res, max_res)\n",
    "\n",
    "select_res = widgets.BoundedFloatText(\n",
    "    value=min_res,  # defaults to the maximum of the minimum and maximum resolutions\n",
    "    min=max_res,\n",
    "    max=max_res,\n",
    "    step=0.00001,  # 1 m\n",
    "    description=\"Resolution:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# combine the widgets into an HBox layout\n",
    "widget_layout = widgets.HBox([select_tile_size, select_res])\n",
    "\n",
    "\n",
    "if min_res == max_res:\n",
    "    print(\n",
    "        f\"{time.ctime()}: Detected only one resolution of {min_res} m from input data.\"\n",
    "    )\n",
    "    select_res.disabled = True\n",
    "\n",
    "    display(select_tile_size)\n",
    "else:\n",
    "    print(\"For reference:\")\n",
    "    print(\n",
    "        f\"{time.ctime()}: Detected minimum (lowest) resolution of {min_res} m from input data.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{time.ctime()}: Detected maximum (highest) resolution of {max_res} m from input data.\"\n",
    "    )\n",
    "\n",
    "    select_res = widgets.BoundedFloatText(\n",
    "        # value=0.00010,  # defaults to 10 m\n",
    "        min=max_res,\n",
    "        step=0.00001,  # 1 m\n",
    "        description=\"Resolution:\",\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # combine the widgets into an HBox layout\n",
    "    widget_layout = widgets.HBox([select_tile_size, select_res])\n",
    "\n",
    "    # display the layout\n",
    "    widget_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80514e",
   "metadata": {},
   "source": [
    "## Select vector file features\n",
    "\n",
    "If you uploaded any features in vector file format, you will be prompted to select which features to use. Vector files may include multiple features, which will need to be encoded as separate bands in the composite/stacked raster used to train the model. If you don't select any features, the function will default to selecting the first feature it finds in the first vector file it searches.\n",
    "\n",
    "It is recommended that you only select the most relevant features to reduce the time and space complexity of the model (how long and how much memory it takes to run). Additionally, too many inputs may cloud the model's ability to learn the relationship between the inputs and the mask.\n",
    "\n",
    "> Note that this is not relevant for raster feature files, which will be processed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55129e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to train on\n",
    "\n",
    "# set tile & pixel size from widget selections\n",
    "TILE_SIZE = select_tile_size.value\n",
    "# PIXEL_SIZE = select_res.value\n",
    "PIXEL_SIZE = 2\n",
    "\n",
    "all_col = []\n",
    "\n",
    "# todo : improve this method instead of forcing an error on tiff files\n",
    "for path in feature_paths:\n",
    "    # print(path, feature)\n",
    "    feature = Path(os.path.basename(path)).stem\n",
    "    try:  # try to open aka check if its a vector\n",
    "        gdf = gpd.read_file(path)\n",
    "        all_col.extend([f\"{feature}: {n}\" for n in gdf.columns])\n",
    "        all_col.remove(f\"{feature}: geometry\")\n",
    "    except Exception as e:\n",
    "        # print(f\"{time.ctime()}: {e}\")\n",
    "        print(\n",
    "            f\"{time.ctime()}: Feature input {path} is already a raster file, no features need to be extracted.\"\n",
    "        )\n",
    "\n",
    "# select columns to keep\n",
    "to_keep = widgets.SelectMultiple(\n",
    "    options=all_col,\n",
    "    # value=[all_col[0]], # defaults to first feature\n",
    "    description=\"Features: \",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "if len(all_col) > 0:\n",
    "    to_keep.value = [all_col[0]]  # defaults to first feature\n",
    "    display(to_keep)\n",
    "else:\n",
    "    print(f\"{time.ctime()}: You do not need to select any input features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16086bea",
   "metadata": {},
   "source": [
    "## Rasterize data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88062978",
   "metadata": {},
   "source": [
    "In order for the model to learn from the data, the input feature data will be encoded in raster bands. Therefore, any input features in vector file format will be rasterized using the data resolution selected above.\n",
    "\n",
    "> Note: Depending on the size of your data, this step may take several minutes to run. In the case of large datasets, it may be more efficient to rasterize the data in a GIS software such as QGIS or ArcGIS and upload the rasterized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04705a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterize masks and selected features if necessary\n",
    "\n",
    "# rasterize masks if necessary\n",
    "try:\n",
    "    print(f\"{time.ctime()}: Attempting to rasterize {mask_path}...\")\n",
    "    gdf = gpd.read_file(mask_path)\n",
    "    # old_path = mask_path\n",
    "    # mask_path = os.path.join(working_dir, \"mask.tif\")\n",
    "    gdf[\"encoding_key\"] = 1\n",
    "    os.remove(mask_path)\n",
    "    gdf.to_file(mask_path)\n",
    "    to_raster(\n",
    "        mask_path, os.path.join(working_dir, \"mask.tif\"), feature_id=\"encoding_key\", pixel_size=select_res.value, windows_shape=(2064, 2064)\n",
    "    )\n",
    "    mask_path = os.path.join(working_dir, \"mask.tif\")\n",
    "except:\n",
    "    print(f\"{time.ctime()}: {mask_path} is already a raster file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd684029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature_ids for the features that need to be turned into bands\n",
    "keeping = [re.findall(r\"\\s(.*)\", s) for s in to_keep.value]\n",
    "keeping = list(itertools.chain.from_iterable(keeping))\n",
    "keeping = list(map(str.lower, keeping))\n",
    "\n",
    "band_paths = []\n",
    "\n",
    "# convert the features to rasters if necessary\n",
    "for feature_path in feature_paths:\n",
    "    try:\n",
    "        gdf = gpd.read_file(feature_path)\n",
    "        gdf.columns = list(map(str.lower, gdf.columns))\n",
    "\n",
    "        for feature in keeping:\n",
    "            if feature in gdf.columns:\n",
    "                fn = feature + \".tif\"\n",
    "                to_raster(\n",
    "                    feature_path,\n",
    "                    os.path.join(working_dir, fn),\n",
    "                    feature_id=feature,\n",
    "                    pixel_size=PIXEL_SIZE,\n",
    "                    windows_shape=(4084, 4084),\n",
    "                )\n",
    "                band_paths.append(os.path.join(working_dir, fn))\n",
    "                print(\n",
    "                    f\"{time.ctime()}: {feature} is in {feature_path}, band saved as {fn}.\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"{time.ctime()}: {feature} is not in {feature_path}\")\n",
    "    except:\n",
    "        print(f\"{time.ctime()}: {feature} is already a raster file.\")\n",
    "        band_paths.append(feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924834ee",
   "metadata": {},
   "source": [
    "## Stack the features into a multiband raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfc36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all rasters are the same resolution & have x & y dims divisible by 2\n",
    "with rasterio.open(mask_path) as mask_ds:\n",
    "    # Loop through the band rasters\n",
    "    for band_path in tqdm(band_paths):\n",
    "        with rasterio.open(band_path) as band_ds:\n",
    "            # Reproject and resample the band raster to match the mask\n",
    "            with rasterio.open(\n",
    "                band_path,\n",
    "                \"w\",\n",
    "                driver=\"GTiff\",\n",
    "                height=mask_ds.height,\n",
    "                width=mask_ds.width,\n",
    "                count=band_ds.count,\n",
    "                dtype=band_ds.dtypes[0],\n",
    "                crs=mask_ds.crs,\n",
    "                transform=mask_ds.transform,\n",
    "                nodata=NO_DATA,\n",
    "            ) as band_reprojected:\n",
    "                reproject(\n",
    "                    source=rasterio.band(band_ds, 1),\n",
    "                    destination=rasterio.band(band_reprojected, 1),\n",
    "                    src_transform=band_ds.transform,\n",
    "                    src_crs=band_ds.crs,\n",
    "                    dst_transform=mask_ds.transform,\n",
    "                    dst_crs=mask_ds.crs,\n",
    "                    resampling=Resampling.bilinear,\n",
    "                )\n",
    "\n",
    "# crop to bounds and save to working directory\n",
    "band_paths_list = es.crop_all(\n",
    "    [mask_path, *band_paths], working_dir, bounds_gpd, overwrite=True\n",
    ")\n",
    "\n",
    "# build a list describing the bands to be stacked in the composite image\n",
    "band_paths = []\n",
    "mask_path = \"\"\n",
    "for path in band_paths_list:\n",
    "    if \"mask\" in path:\n",
    "        mask_path = path\n",
    "    else:\n",
    "        band_paths.append(path)\n",
    "\n",
    "# create a save path for the stacked bands in the working dir\n",
    "stack_path = os.path.join(working_dir, \"stack.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata of first band\n",
    "with rio.open(band_paths[0]) as src:\n",
    "    meta = src.meta\n",
    "\n",
    "# count the number of bands in the input layers\n",
    "band_count = 0\n",
    "for layer in band_paths:\n",
    "    with rio.open(layer, \"r\") as src:\n",
    "        band_count += src.count\n",
    "\n",
    "# update meta to reflect the number of layers\n",
    "meta.update(count=band_count)\n",
    "\n",
    "# read each layer/band and write it to stack using rasterio\n",
    "BANDS = {}\n",
    "id = 1\n",
    "with rio.open(stack_path, \"w\", **meta) as dst:\n",
    "    for layer in band_paths:\n",
    "        with rio.open(layer, \"r\") as src:\n",
    "            for band in range(1, src.count + 1):\n",
    "                l = Path(layer).stem\n",
    "                print(\n",
    "                    f\"{time.ctime()}: Writing band {band} from {l} to composite raster.\"\n",
    "                )\n",
    "                dst.write_band(id, src.read(band))\n",
    "                BANDS[f\"{os.path.basename(l)}_BAND-{band}\"] = id - 1\n",
    "                id += 1\n",
    "        src.close()\n",
    "\n",
    "print(f\"{time.ctime()}: Composite raster saved as {stack_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bca5f8",
   "metadata": {},
   "source": [
    "## Data Tiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffaee0",
   "metadata": {},
   "source": [
    "### Create tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d469550",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create a spatial data generator for the mask and data stack\n",
    "mask_sgd = SpatialDataGenerator(source=mask_path, interleave=\"pixel\")\n",
    "data_sgd = SpatialDataGenerator(source=stack_path, interleave=\"pixel\")\n",
    "\n",
    "# create regularly gridded tiles that cover the bounds of the mask w no overlap\n",
    "tile_bounds_gdf = mask_sgd.regular_grid(TILE_SIZE, TILE_SIZE, overlap=0, units=\"pixels\")\n",
    "\n",
    "# create a geodataframe of the bounds of the mask\n",
    "tiles_gdf = tile_bounds_gdf[tile_bounds_gdf.within(bounds_gpd.unary_union)].copy()\n",
    "\n",
    "# print the number of tiles created\n",
    "print(f\"{time.ctime()}: Created {len(tiles_gdf)} tiles size {TILE_SIZE}x{TILE_SIZE}.\")\n",
    "\n",
    "# plot the tiles\n",
    "tiles_ax = tiles_gdf.boundary.plot(figsize=(8, 8), edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "# add a title to the plot\n",
    "tiles_ax.set_title(\"Tiled Area of Interest\")\n",
    "\n",
    "# add axis labels\n",
    "tiles_ax.set_xlabel(\"Longitude\")\n",
    "tiles_ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# plot the mask polygons underneath the tiles\n",
    "try:\n",
    "    # add the masks to the plot\n",
    "    mask_polygons.plot(ax=tiles_ax, color=\"red\", edgecolor=\"red\")\n",
    "except:\n",
    "    # add the raster to the plot\n",
    "    with rio.open(mask_path) as src:\n",
    "        show(src, ax=tiles_ax, cmap=\"gray\")\n",
    "\n",
    "# plot the bounds of the area of interest\n",
    "bounds_gpd.boundary.plot(ax=tiles_ax, color=None, linewidth=2)\n",
    "\n",
    "# add a basemap to the plot\n",
    "cx.add_basemap(tiles_ax, source=cx.providers.Esri.WorldTopoMap, crs=CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70027e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Extract images and masks from tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceeff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data to be in the correct shape for the model\n",
    "def img_reshape(arr):\n",
    "    return arr.reshape(TILE_SIZE, TILE_SIZE, -1)\n",
    "\n",
    "\n",
    "# ensure that the data is in the correct shape for the model\n",
    "data_sgd.add_preprocess_callback(\"reshape\", img_reshape)\n",
    "mask_sgd.add_preprocess_callback(\"reshape\", img_reshape)\n",
    "\n",
    "# create generators for the input data\n",
    "X_gen = data_sgd.flow_from_dataframe(tiles_gdf, TILE_SIZE, TILE_SIZE, batch_size=1)\n",
    "Y_gen = mask_sgd.flow_from_dataframe(tiles_gdf, TILE_SIZE, TILE_SIZE, batch_size=1)\n",
    "\n",
    "# n = len(tiles_gdf) # number of tiles\n",
    "\n",
    "\n",
    "# define a function to unpack a generator\n",
    "def unpack_gen(gen):\n",
    "    stack = []  # create an empty list to store the data\n",
    "    while True:  # loop until the generator is exhausted\n",
    "        try:\n",
    "            stack.append(next(gen))  # append the next batch (1) to the list\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return np.stack(stack, axis=1)  # stack the list of batches into a single array\n",
    "\n",
    "\n",
    "X = unpack_gen(X_gen)  # unpack the data generator\n",
    "Y = unpack_gen(Y_gen)  # unpack the mask generator\n",
    "\n",
    "# reshape the data to be in the correct format for the model\n",
    "X = X[0, ...].astype(np.float32)\n",
    "Y = Y[0, ...].astype(np.float32)\n",
    "\n",
    "Y = np.where(Y > 0, 1, 0)  # binarize the mask\n",
    "\n",
    "# print(X.shape, Y.shape)\n",
    "# np.unique(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8c1a6",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to the correct data type\n",
    "X = np.array(X, dtype=np.float32)\n",
    "Y = np.array(Y, dtype=np.uint8)\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "    normalized_images = np.zeros_like(images, dtype=np.float32)\n",
    "    num_images, _, _, channels = images.shape\n",
    "\n",
    "    # Reshape the images to a 2D array for vectorized normalization\n",
    "    reshaped_images = images.reshape(num_images, -1)\n",
    "\n",
    "    # Perform per-channel normalization to [0, 1] range\n",
    "    for c in range(channels):\n",
    "        channel = images[:, :, :, c]\n",
    "        normalized_channel = cv2.normalize(\n",
    "            channel, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F\n",
    "        )\n",
    "        normalized_images[:, :, :, c] = normalized_channel\n",
    "\n",
    "    return normalized_images\n",
    "\n",
    "\n",
    "# normalize the images\n",
    "X_norm = normalize_images(X)\n",
    "\n",
    "print(\n",
    "    f\"{time.ctime()}: Completed per-channel, per-image normalization of {len(X_norm)} images.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3cbd2",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Check to make sure things look good visually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1648117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three dropdowns to select the band to display\n",
    "options = list(BANDS.keys())\n",
    "options.insert(0, \"None\")\n",
    "band1 = widgets.Dropdown(options=options, description=\"Band 1\")\n",
    "band2 = widgets.Dropdown(options=options, description=\"Band 2\")\n",
    "band3 = widgets.Dropdown(options=options, description=\"Band 3\")\n",
    "\n",
    "# display the dropdowns in boxes\n",
    "b = widgets.VBox([band1, band2, band3])\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2371c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = []\n",
    "for c in b.children:\n",
    "    if c.value != \"None\":\n",
    "        bands.append(BANDS[c.value])\n",
    "\n",
    "r = 5\n",
    "if not len(bands) == 0:\n",
    "    n = np.random.randint(0, len(X), r)\n",
    "\n",
    "    # plot r random tiles in separate images\n",
    "    for i in range(r):\n",
    "        spectral.imshow(\n",
    "            X[n[i], ...],\n",
    "            bands=bands,\n",
    "            stretch=True,\n",
    "            title=f\"Tile {n[i]} with Bands {bands}\",\n",
    "            figsize=(3, 3),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = X.shape[-1]  # get the number of channels in the input data\n",
    "num_images = 5  # number of random images to display\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "for i in range(num_images):\n",
    "    n = np.random.randint(0, len(X))\n",
    "\n",
    "    ax = fig.add_subplot(2, num_images, i + 1)\n",
    "    ax.imshow(Y[n, :, :, 0], cmap=\"Greys_r\")\n",
    "    ax.set_title(f\"Mask {n}\")\n",
    "\n",
    "    # images\n",
    "    # for j in range(num_channels):\n",
    "    ax = fig.add_subplot(2, num_images, num_images + i + 1)\n",
    "    ax.imshow(X[n, :, :, 0], cmap=\"viridis\")\n",
    "    ax.set_title(f\"Img. {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979d4e",
   "metadata": {},
   "source": [
    "## Split Data & Set Parameters\n",
    "\n",
    "- add info about deciding split ratios and the other params to be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573771d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = widgets.FloatSlider(\n",
    "    value=0.65,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Training %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    # orientation=\"vertical\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "validation_size = widgets.FloatSlider(\n",
    "    value=0.25,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Validation %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    # orientation=\"vertical\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "test_size = widgets.FloatSlider(\n",
    "    value=0.10,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Test %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    # orientation=\"vertical\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "select_epochs = widgets.IntSlider(\n",
    "    value=25,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description=\"# Epochs:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\"d\",\n",
    ")\n",
    "\n",
    "# should be a factor of 2 to take advantage of the GPU resources\n",
    "select_batch_size = widgets.Dropdown(\n",
    "    options=[8, 16, 32, 64, 128],\n",
    "    value=16,\n",
    "    description=\"Batch Size:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "widgets.VBox([train_size, validation_size, test_size, select_epochs, select_batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = select_epochs.value  # default = 50\n",
    "BATCH_SIZE = select_batch_size.value  # default = 32\n",
    "\n",
    "# validate the splits\n",
    "# assert int(train_size.value + validation_size.value + test_size.value) == 1\n",
    "\n",
    "TRAIN_SIZE = train_size.value\n",
    "VALID_SIZE = validation_size.value\n",
    "TEST_SIZE = test_size.value\n",
    "\n",
    "# train test split\n",
    "print(f\"{time.ctime()}: Splitting data into train, validation, and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=TEST_SIZE, shuffle=True, random_state=SEED\n",
    ")\n",
    "\n",
    "# extricate the validation set from the training set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=VALID_SIZE, shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{time.ctime()}: Dataset sizes:\")\n",
    "print(f\"Training: {len(X_train)}\")\n",
    "print(f\"Validation: {len(X_val)}\")\n",
    "print(f\"Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49d25d",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- add info about deciding augmentation params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ea22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fliplr = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Flip L/R %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "flipud = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Flip U/D %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "transX = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Translate X %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "transY = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Translate Y %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "scaleX = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Scale X %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "scaleY = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Scale Y %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "add_noise = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Noise %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "hist_eq = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Hist. Eq. %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "box = widgets.VBox([fliplr, flipud, transX, transY, scaleX, scaleY, hist_eq])\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_images_with_mask(X, Y):\n",
    "    filtered_X = []\n",
    "    filtered_Y = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if np.any(Y[i] != 0):  # Check if any pixel in the mask is non-zero\n",
    "            filtered_X.append(X[i])\n",
    "            filtered_Y.append(Y[i])\n",
    "\n",
    "    return filtered_X, filtered_Y\n",
    "\n",
    "\n",
    "X_pos, Y_pos = filter_images_with_mask(X_train, Y_train)\n",
    "\n",
    "print(f\"{time.ctime()}: Found {len(X_pos)} images with masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment all images or just the ones with masks (objs of interest)\n",
    "augment_all = widgets.RadioButtons(\n",
    "    options=[\"Augment only images with masks\", \"Augment all images\"],\n",
    "    description=\"Augment all images or just the ones with masks?\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# choose whether to concatenate augmented or replace the original images\n",
    "concat_replace = widgets.RadioButtons(\n",
    "    options=[\"Concatenate\", \"Replace\"],\n",
    "    description=\"Replace or concatenate to original dataset?\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "widgets.VBox([augment_all, concat_replace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.seed(SEED)\n",
    "\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        iaa.Fliplr(fliplr.value),  # LR flip\n",
    "        iaa.Flipud(flipud.value),  # UD flip\n",
    "        iaa.Sometimes(\n",
    "            transX.value, iaa.TranslateX(percent=(-0.1, 0.1))\n",
    "        ),  # x translation\n",
    "        iaa.Sometimes(\n",
    "            transY.value, iaa.TranslateY(percent=(-0.1, 0.1))\n",
    "        ),  # y translation\n",
    "        iaa.Sometimes(scaleX.value, iaa.ScaleX(scale=(0.8, 1.2))),  # x scale\n",
    "        iaa.Sometimes(scaleY.value, iaa.ScaleY(scale=(0.8, 1.2))),  # y scale\n",
    "        # iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255), per_channel=True),  # add gaussian noise\n",
    "        # iaa.Sometimes(\n",
    "        #     hist_eq.value, iaa.AllChannelsHistogramEqualization()\n",
    "        # ),  # histogram equalization\n",
    "    ]\n",
    ")\n",
    "\n",
    "if augment_all.value == \"Augment all images\":\n",
    "    print(f\"{time.ctime()}: Augmenting all images...\")\n",
    "    X_aug, Y_aug = seq(images=X, segmentation_maps=Y)\n",
    "else:\n",
    "    print(f\"{time.ctime()}: Augmenting images with masks...\")\n",
    "    X_aug, Y_aug = seq(images=X_pos, segmentation_maps=Y_pos)\n",
    "\n",
    "if concat_replace.value == \"Concatenate\":\n",
    "    print(f\"{time.ctime()}: Concatenating augmented images to original dataset...\")\n",
    "    X = np.concatenate((X, X_aug), axis=0)\n",
    "    Y = np.concatenate((Y, Y_aug), axis=0)\n",
    "else:\n",
    "    print(f\"{time.ctime()}: Replacing original dataset with augmented images...\")\n",
    "    X = X_aug\n",
    "    Y = Y_aug\n",
    "\n",
    "print(\n",
    "    f\"{time.ctime()}: Augmentation completed. The dimension of the augmented training dataset is {X.shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdcd4d",
   "metadata": {},
   "source": [
    "## Convert to *tf.data.Dataset* format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the numpy arrays into tensorflow datasets\n",
    "print(f\"{time.ctime()}: Transforming numpy arrays into tensorflow datasets...\")\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "print(f\"{time.ctime()}: Batching and parallelizing the datasets...\")\n",
    "# batch the data, drop the remainder, & parallelize according to available resources\n",
    "train_ds = train_ds.batch(\n",
    "    BATCH_SIZE, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.batch(\n",
    "    BATCH_SIZE, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "test_ds = test_ds.batch(\n",
    "    BATCH_SIZE, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# use all of data points, 1 batch size at a time\n",
    "STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE\n",
    "print(f\"{time.ctime()}: Calculated steps per epoch to be: {STEPS_PER_EPOCH}\")\n",
    "\n",
    "# calculate validation steps\n",
    "VALIDATION_STEPS = len(X_val) // BATCH_SIZE\n",
    "print(f\"{time.ctime()}: Calculated validation steps to be: {VALIDATION_STEPS}\")\n",
    "\n",
    "print(f\"{time.ctime()}: Batching and parallelization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ceed3",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbad984",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build a unet\n",
    "# def bottleneck(inputs, n):\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(conv)\n",
    "\n",
    "#     return conv\n",
    "\n",
    "\n",
    "# def downsample(inputs, n):\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(conv)\n",
    "#     pool = MaxPooling2D((2, 2))(conv)\n",
    "#     # pool = Dropout(0.25)(pool)\n",
    "\n",
    "#     return conv, pool, n * 2\n",
    "\n",
    "\n",
    "# def upsample(inputs, residual, n):\n",
    "#     n = n // 2\n",
    "#     deconv = Conv2DTranspose(n, (3, 3), strides=(2, 2), padding=\"same\")(inputs)\n",
    "#     uconv = concatenate([deconv, residual])\n",
    "#     # uconv = Dropout(0.5)(uconv)\n",
    "#     uconv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(uconv)\n",
    "#     uconv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(uconv)\n",
    "\n",
    "#     return uconv, n\n",
    "\n",
    "\n",
    "# def build_unet(input_shape, n=32):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "\n",
    "#     # downsample\n",
    "#     conv1, pool1, n = downsample(inputs, n)\n",
    "#     conv2, pooreg, n = downsample(pool1, n)\n",
    "#     conv3, pool3, n = downsample(pool2, n)\n",
    "#     conv4, pool4, n = downsample(pool3, n)\n",
    "\n",
    "#     # bottleneck\n",
    "#     conv5 = bottleneck(pool4, n)\n",
    "\n",
    "#     # upsample\n",
    "#     uconv4, n = upsample(conv5, conv4, n)\n",
    "#     uconv3, n = upsample(uconv4, conv3, n)\n",
    "#     uconv2, n = upsample(uconv3, conv2, n)\n",
    "#     uconv1, n = upsample(uconv2, conv1, n)\n",
    "\n",
    "#     outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(uconv1)\n",
    "\n",
    "#     return Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b657c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet w batch norm & dropout\n",
    "reg = tf.keras.regularizers.l1_l2()\n",
    "\n",
    "\n",
    "def double_conv_block(x, n_filters):\n",
    "    # conv2D then ReLU activation\n",
    "    x = Conv2D(\n",
    "        n_filters,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        # kernel_regularizer=reg,\n",
    "    )(x)\n",
    "    # conv2D then ReLU activation\n",
    "    x = Conv2D(\n",
    "        n_filters,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        # kernel_regularizer=reg,\n",
    "    )(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)  # feature map\n",
    "    f = BatchNormalization()(f)  # batch normalization\n",
    "    p = MaxPool2D(2)(f)  # pooled feature map\n",
    "    p = Dropout(0.2)(p)  # dropout\n",
    "\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    # upsample\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(\n",
    "        n_filters,\n",
    "        2,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        # kernel_regularizer=reg,\n",
    "    )(x)\n",
    "    # x = Conv2DTranspose(filters=n_filters, kernel_size=3, strides=(2, 2), padding=\"same\")(x)\n",
    "    # concatenate\n",
    "    x = concatenate([x, conv_features])\n",
    "    # dropout\n",
    "    x = Dropout(0.2)(x)\n",
    "    # Conv2D twice with ReLU activation\n",
    "    x = double_conv_block(x, n_filters)\n",
    "    # batch normalization\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet(input_shape, output_channels, depth=5, base_filters=64):\n",
    "    # inputs\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # encoder: contracting path - downsample\n",
    "    skips = []\n",
    "    filters = []\n",
    "    p = inputs\n",
    "    for i in range(depth):\n",
    "        nf = base_filters * 2**i\n",
    "        f, p = downsample_block(p, nf)\n",
    "        skips.append(f)\n",
    "        filters.append(nf)\n",
    "\n",
    "    # bottleneck\n",
    "    bottleneck = double_conv_block(p, base_filters * 2**depth)\n",
    "\n",
    "    # decoder: expanding path - upsample\n",
    "    u = bottleneck\n",
    "    for i in reversed(range(depth)):\n",
    "        u = upsample_block(u, skips[i], filters[i])\n",
    "\n",
    "    # check if output channels is 1 (binary) or > 1 (multiclass)\n",
    "    if output_channels == 1:\n",
    "        a_func = \"sigmoid\"\n",
    "        print(\n",
    "            f\"{time.ctime()}: Using sigmoid activation function for binary semantic segmentation.\"\n",
    "        )\n",
    "    else:\n",
    "        a_func = \"softmax\"\n",
    "        print(\n",
    "            f\"{time.ctime()}: Using softmax activation function for multiclass semantic segmentation.\"\n",
    "        )\n",
    "\n",
    "    # outputs\n",
    "    outputs = Conv2D(output_channels, 1, padding=\"same\", activation=a_func)(u)\n",
    "\n",
    "    # U-Net model with Keras Functional API\n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896adfb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Net(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        output_channels,\n",
    "        filters=32,\n",
    "        depth=3,\n",
    "        dilation_rates=[1],\n",
    "        reduction_ratio=16,\n",
    "        name=\"model\",\n",
    "    ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_channels = output_channels\n",
    "        self.filters = filters\n",
    "        self.depth = depth\n",
    "        self.dilation_rates = dilation_rates\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.name = name\n",
    "        self.skips = []\n",
    "        self.inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    def channel_attention(self, inputs):\n",
    "        # channel attention\n",
    "        channels = inputs.shape[-1]\n",
    "        avg_pool = layers.GlobalAveragePooling2D()(inputs)\n",
    "        fc1 = layers.Dense(channels // self.reduction_ratio)(avg_pool)\n",
    "        fc1 = layers.ReLU()(fc1)\n",
    "        fc2 = layers.Dense(channels)(fc1)\n",
    "        fc2 = layers.Activation(\"sigmoid\")(fc2)\n",
    "        reshaped = layers.Reshape((1, 1, channels))(fc2)\n",
    "        return layers.Multiply()([inputs, reshaped])\n",
    "\n",
    "    def spatial_attention(self, inputs, kernel_size=5):\n",
    "        # spatial attention\n",
    "        spatial_attn = layers.Conv2D(1, kernel_size, padding=\"same\")(inputs)\n",
    "        spatial_attn = layers.Activation(\"sigmoid\")(spatial_attn)\n",
    "        return layers.Multiply()([inputs, spatial_attn])\n",
    "\n",
    "    def CBAM_block(self, inputs):\n",
    "        # conv\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(inputs)\n",
    "        # bn + relu\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # channel attention\n",
    "        channel_attention_output = self.channel_attention(inputs, self.reduction_ratio)\n",
    "        # spatial attention\n",
    "        spatial_attention_output = self.spatial_attention(inputs)\n",
    "        # merge\n",
    "        output = layers.Add()([channel_attention_output, spatial_attention_output])\n",
    "        return output\n",
    "\n",
    "    def pyramid_pooling_block(self, inputs, bin_sizes=[1, 2, 3, 6]):\n",
    "        h, w, c = inputs.shape[1:]\n",
    "        pooled_outputs = []  # store pooled output tensors for each bin size\n",
    "        # remove invalid bin sizes\n",
    "        bin_sizes = [size for size in bin_sizes if min(h, w) % size == 0]\n",
    "        # print(\"bin sizes\", bin_sizes)\n",
    "        for size in bin_sizes:  # iterate over each bin size\n",
    "            x = layers.AveragePooling2D(pool_size=(h // size, w // size))(inputs)\n",
    "            x = layers.Conv2D(self.filters, 1, padding=\"same\")(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            # resize so all pooled output tensors have the same shape\n",
    "            x = layers.Reshape((h, w, -1))(x)\n",
    "            pooled_outputs.append(x)\n",
    "        # concatenate pooled output tensors\n",
    "        x = layers.Concatenate(axis=-1)(pooled_outputs)\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 1, padding=\"same\")(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        return x\n",
    "\n",
    "    # residual block with atrous convolutions\n",
    "    def residual_block(self, inputs):\n",
    "        x = inputs\n",
    "        stack = [x]\n",
    "        for rate in self.dilation_rates:\n",
    "            # conv block 1\n",
    "            x = layers.Conv2D(self.filters, 3, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # conv block 2\n",
    "            x = layers.Conv2D(self.filters, 1, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # apply channel-wise attention\n",
    "            se = self.channel_attention(x)\n",
    "            # add to stack to combine parallel atrous convolutions\n",
    "            stack.append(se)\n",
    "        # print([s.shape for s in stack])\n",
    "        x = layers.Add()(stack)\n",
    "        return x\n",
    "\n",
    "    def encoding_block(self, inputs):\n",
    "        # residual block\n",
    "        x = self.residual_block(inputs)\n",
    "        # enhance skip with spatial attention\n",
    "        skip = self.spatial_attention(x)\n",
    "        # update filters\n",
    "        self.filters *= 2\n",
    "        # downsampling with conv2d\n",
    "        x = layers.Conv2D(self.filters, 1, 2, padding=\"same\")(skip)\n",
    "        return x, skip\n",
    "\n",
    "    def decoding_block(self, inputs):\n",
    "        # calculate new number of filters\n",
    "        self.filters //= 2\n",
    "        # upsample the feature maps\n",
    "        x = layers.UpSampling2D()(inputs)\n",
    "        # concatenate skip connection from the corresponding encoding block\n",
    "        x = layers.Concatenate(axis=-1)([x, self.skips.pop()])\n",
    "        # residual block\n",
    "        stack = []\n",
    "        for rate in self.dilation_rates:\n",
    "            # conv block 1\n",
    "            x = layers.Conv2D(self.filters, 3, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # conv block 2\n",
    "            x = layers.Conv2D(self.filters, 1, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # apply channel-wise attention\n",
    "            se = self.channel_attention(x)\n",
    "            # add to stack to combine parallel atrous convolutions\n",
    "            stack.append(se)\n",
    "        # print([s.shape for s in stack])\n",
    "        x = layers.Add()(stack)\n",
    "        return x\n",
    "\n",
    "    def bridge_block(self, inputs):\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(inputs)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # psp block\n",
    "        # x = self.PSPPooling_block(inputs)\n",
    "\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # cbam block\n",
    "        # x = self.CBAM_block(x)\n",
    "\n",
    "        # residual block\n",
    "        # x = self.residual_block(x)\n",
    "        return x\n",
    "\n",
    "    def build(self):\n",
    "        # input block - inc to initial desired filter size\n",
    "        x = self.inputs\n",
    "        x = layers.Conv2D(self.filters, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        self.skips.append(x)\n",
    "\n",
    "        x = self.residual_block(x)\n",
    "\n",
    "        # encoding path\n",
    "        for _ in range(self.depth):\n",
    "            # print(self.filters, x.shape)\n",
    "            x, skip = self.encoding_block(x)\n",
    "            self.skips.append(skip)\n",
    "\n",
    "        # bridge\n",
    "        # print(\"bridge\", self.filters)\n",
    "        x = self.bridge_block(x)\n",
    "        # x = self.CBAM_block(x)\n",
    "\n",
    "        # decoding path\n",
    "        for _ in range(self.depth):\n",
    "            x = self.decoding_block(x)\n",
    "            # print(self.filters, x.shape)\n",
    "\n",
    "        # check if multiclass or binary classification\n",
    "        if self.output_channels > 1:\n",
    "            a_fn = \"softmax\"\n",
    "        else:\n",
    "            a_fn = \"sigmoid\"\n",
    "\n",
    "        # psp block\n",
    "\n",
    "        # get final skip connection from stack\n",
    "        x = layers.Concatenate(axis=-1)([x, self.skips.pop()])\n",
    "        # output segmentation map\n",
    "        outputs = layers.Conv2D(\n",
    "            self.output_channels, 1, padding=\"same\", activation=a_fn\n",
    "        )(x)\n",
    "        print(self.input_shape, outputs.shape)\n",
    "        return tf.keras.Model(inputs=self.inputs, outputs=outputs, name=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30517a6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# todo add widgets for hyperparameter tuning (sliders galore)\n",
    "\n",
    "# hyperparams to think about:\n",
    "\n",
    "# - train, valid, test size, tile size, pixel size\n",
    "# - epochs, batch size, optimizer, loss functions, accuracy metrics\n",
    "# - call backs, early stopping, patience\n",
    "# select loss function\n",
    "\n",
    "\n",
    "# select optimizer\n",
    "\n",
    "\n",
    "# select metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f193ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-5):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return dice_coeff\n",
    "\n",
    "\n",
    "def weighted_dice_loss(class_proportions):\n",
    "    def loss(y_true, y_pred):\n",
    "        dice = dice_coefficient(y_true, y_pred)\n",
    "\n",
    "        # compute weights based on class proportions\n",
    "        weights = tf.constant(class_proportions, dtype=tf.float32)\n",
    "\n",
    "        # index 0: background; 1: foreground\n",
    "        weighted_dice = tf.multiply(y_true * weights[1], dice) + tf.multiply(\n",
    "            (1.0 - y_true) * weights[0], dice\n",
    "        )\n",
    "        return 1.0 - tf.reduce_mean(weighted_dice)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # Convert labels to tensors\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        # Calculate binary crossentropy\n",
    "        binary_cross_entropy = BinaryCrossentropy()(y_true, y_pred)\n",
    "\n",
    "        # Calculate focal loss\n",
    "        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        focal_loss = modulating_factor * binary_cross_entropy\n",
    "\n",
    "        # Apply class balancing\n",
    "        weighted_focal_loss = alpha * focal_loss\n",
    "\n",
    "\n",
    "        return tf.reduce_mean(weighted_focal_loss)\n",
    "\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays to tensors with permuted dimensions (N, C, H, W)\n",
    "X_train_tensor = torch.from_numpy(X_train).permute(0, 3, 1, 2)\n",
    "Y_train_tensor = torch.from_numpy(Y_train).permute(0, 3, 1, 2)\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "# Create a train dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# print shapes\n",
    "print(\"X_train_tensor:\", X_train_tensor.shape)\n",
    "print(\"Y_train_tensor:\", Y_train_tensor.shape)\n",
    "\n",
    "# repeat for validation data loader\n",
    "X_val_tensor = torch.from_numpy(X_val).permute(0, 3, 1, 2)\n",
    "Y_val_tensor = torch.from_numpy(Y_val).permute(0, 3, 1, 2)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"\\nX_val_tensor:\", X_val_tensor.shape)\n",
    "print(\"Y_val_tensor:\", Y_val_tensor.shape)\n",
    "\n",
    "# repeat for test data loader\n",
    "X_test_tensor = torch.from_numpy(X_test).permute(0, 3, 1, 2)\n",
    "Y_test_tensor = torch.from_numpy(Y_test).permute(0, 3, 1, 2)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"\\nX_test_tensor:\", X_test_tensor.shape)\n",
    "print(\"Y_test_tensor:\", Y_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filters=32):\n",
    "        super(UNet, self).__init__()\n",
    "        self.float()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(filters, out_channels, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()  # function\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.decoder(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26924e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Focal Loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def intersection_over_union(predicted, target, epsilon=1e-8):\n",
    "    intersection = (predicted * target).sum(dim=(1, 2))  # Calculate the intersection for each sample\n",
    "    union = (predicted + target).sum(dim=(1, 2)) - intersection  # Calculate the union for each sample\n",
    "\n",
    "    iou = (intersection + epsilon) / (union + epsilon)  # Calculate IoU for each sample, adding epsilon to avoid division by zero\n",
    "\n",
    "    # Return the mean IoU across all samples\n",
    "    return iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine, Events\n",
    "\n",
    "# Assuming you already have your model defined\n",
    "in_channels = X_train_tensor.shape[1]\n",
    "model = UNet(in_channels, 1)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# criterion = FocalLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # You can adjust the learning rate\n",
    "threshold = 0.5\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, IoU, Precision, Recall\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.engine import create_supervised_evaluator\n",
    "\n",
    "# # Assuming you have a dataset class named 'YourDataset' and the train and validation datasets are initialized\n",
    "# train_dataset = YourDataset(train=True)\n",
    "# val_dataset = YourDataset(train=False)\n",
    "\n",
    "# # Assuming you have a DataLoader for each dataset\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count(), train=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count(), train=False)\n",
    "\n",
    "# Assuming you already have your model defined\n",
    "in_channels = X_train_tensor.shape[1]\n",
    "model = UNet(in_channels, 1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define your Ignite engine functions for training and validation\n",
    "def update_model(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = model(x)\n",
    "    # y_pred = (y_pred > 0.5).float()\n",
    "    loss = criterion(y_pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "\n",
    "# Create the Ignite engines\n",
    "trainer = Engine(update_model)\n",
    "evaluator = create_supervised_evaluator(model, metrics={\n",
    "    'accuracy': Accuracy(),\n",
    "    'loss': Loss(criterion),\n",
    "    # 'iou': IoU(),\n",
    "    'precision': Precision(),\n",
    "    'recall': Recall()\n",
    "}, device=device)\n",
    "\n",
    "# Attach handlers for metrics computation\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=BATCH_SIZE//5))\n",
    "def log_training_loss(engine):\n",
    "    print(\"Epoch[{}] Iteration[{}]: Loss: {:.4f}\".format(engine.state.epoch, engine.state.iteration, engine.state.output))\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch_results(engine):\n",
    "    evaluator.run(val_dataloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.4f}  Avg loss: {:.4f}  Avg Precision: {:.4f}  Avg Recall: {:.4f}\"\n",
    "          .format(engine.state.epoch, metrics['accuracy'], metrics['loss'], metrics['precision'], metrics['recall']))\n",
    "\n",
    "# Attach ModelCheckpoint and EarlyStopping handlers\n",
    "checkpoint_handler = ModelCheckpoint(dirname='checkpoints', filename_prefix='unet_checkpoint', n_saved=3, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'model': model})\n",
    "early_stopping_handler = EarlyStopping(patience=2, score_function=lambda engine: -engine.state.metrics['loss'], trainer=trainer)\n",
    "evaluator.add_event_handler(Events.COMPLETED, early_stopping_handler)\n",
    "\n",
    "# Run the training loop\n",
    "trainer.run(train_dataloader, max_epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71701fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    x, y = batch\n",
    "    # x = x.to(device)\n",
    "    # y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y.float())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    binary_predictions = (y_pred > 0.5).float()\n",
    "\n",
    "    return {\"y_pred\": binary_predictions, \"y\": y, 'loss': loss.item()}\n",
    "\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    x, y = batch\n",
    "    # x = x.to(device)\n",
    "    # y = y.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y.float())\n",
    "        binary_predictions = (y_pred > 0.5).float()\n",
    "\n",
    "    return {\"y_pred\": binary_predictions, \"y\": y, 'loss': loss.item()}\n",
    "\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "validator = Engine(validation_step)\n",
    "\n",
    "# define metrics\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "# Accuracy and loss metrics are defined\n",
    "metrics = {\"accuracy\": Accuracy(is_multilabel=False), \"loss\": Loss(criterion)}\n",
    "\n",
    "# Attach metrics to the evaluator\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(validator, name)\n",
    "\n",
    "# todo: define checkpoints and early stopping\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# define evaluators\n",
    "from ignite.engine import create_supervised_evaluator\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics)\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics)\n",
    "\n",
    "\n",
    "# define events\n",
    "@trainer.on(Events.EPOCH_STARTED)\n",
    "def print_epoch(engine):\n",
    "    print(f\"Epoch: {engine.state.epoch}/{engine.state.max_epochs}\")\n",
    "\n",
    "\n",
    "# @trainer.on(Events.ITERATION_COMPLETED(every=BATCH_SIZE//5))\n",
    "# def log_batch_metrics(engine):\n",
    "#     print(\n",
    "#         f'  Batch {engine.state.iteration}/{len(train_dataloader)} - {\", \".join([f\"{key} = {value:.4f}\" for key, value in metrics.items()])}'\n",
    "#     )\n",
    "\n",
    "# @trainer.on(Events.EPOCH_COMPLETED)\n",
    "# def log_epoch_metrics(engine):\n",
    "#     train_evaluator.run(train_dataloader)\n",
    "#     metrics_output = train_evaluator.state.metrics\n",
    "#     print(\n",
    "#         f'Training Results - {\", \".join([f\"{key} = {value:.4f}\" for key, value in metrics_output.items()])}'\n",
    "#     )\n",
    "\n",
    "#     validation_evaluator.run(val_dataloader)\n",
    "#     metrics_output = validation_evaluator.state.metrics\n",
    "#     print(\n",
    "#         f'Validation Results - {\", \".join([f\"{key} = {value:.4f}\" for key, value in metrics_output.items()])}'\n",
    "#     )\n",
    "\n",
    "# ### ALT\n",
    "\n",
    "# @trainer.on(Events.ITERATION_COMPLETED(every=BATCH_SIZE//5))\n",
    "# def log_batch_metrics(engine):\n",
    "#     print(\n",
    "#         f'  Batch {engine.state.iteration}/{len(train_dataloader)} - ' +\n",
    "#         f'Accuracy = {metrics[\"accuracy\"].compute():.4f}, ' +\n",
    "#         f'Loss = {metrics[\"loss\"].compute():.4f}'\n",
    "# )\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch_metrics(engine):\n",
    "    train_evaluator.run(train_dataloader)\n",
    "    metrics_output = train_evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Training Results - \"\n",
    "        + f'Accuracy = {metrics_output[\"accuracy\"]:.4f}, '\n",
    "        + f'Loss = {metrics_output[\"loss\"]:.4f}'\n",
    "    )\n",
    "\n",
    "    validation_evaluator.run(val_dataloader)\n",
    "    metrics_output = validation_evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Validation Results - \"\n",
    "        + f'Accuracy = {metrics_output[\"accuracy\"]:.4f}, '\n",
    "        + f'Loss = {metrics_output[\"loss\"]:.4f}'\n",
    "    )\n",
    "\n",
    "# train the model\n",
    "trainer.run(train_dataloader, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80545b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function\n",
    "def update_fn(engine, batch):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # unpack the batch\n",
    "    inputs, targets = batch\n",
    "    outputs = model(inputs.float())\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(outputs, targets.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return {'y_pred': outputs, 'y': targets, 'loss': loss.item()}\n",
    "\n",
    "\n",
    "trainer = Engine(update_fn)\n",
    "\n",
    "# Attach running average metrics to the trainer\n",
    "metrics = [\"loss\", \"iou\"] #, \"precision\"]  # , \"precision\", \"recall\", \"F1\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print that the epoch has started\n",
    "@trainer.on(Events.EPOCH_STARTED)\n",
    "def print_epoch(engine):\n",
    "    print(f\"Epoch {engine.state.epoch}/{engine.state.max_epochs}:\")\n",
    "\n",
    "\n",
    "# Print batch metrics\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=15))\n",
    "def log_training_metrics(engine):\n",
    "    metrics = engine.state.metrics\n",
    "\n",
    "    # print all metrics on same line determined from metrics using loop\n",
    "    print(\n",
    "        f'  Batch {engine.state.iteration}/{len(train_dataloader)} - {\", \".join([f\"{key} = {value:.4f}\" for key, value in metrics.items()])}'\n",
    "    )\n",
    "\n",
    "\n",
    "# Print average loss and IoU for the epoch\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch_results(engine):\n",
    "    # print epoch num and metrics\n",
    "    metrics = engine.state.metrics\n",
    "\n",
    "    # print all metrics on same line determined from metrics using loop\n",
    "    print(\n",
    "        f'Epoch {engine.state.epoch}/{engine.state.max_epochs} - {\", \".join([f\"{key} = {value:.4f}\" for key, value in metrics.items()])}'\n",
    "    )\n",
    "\n",
    "\n",
    "# Run the trainer for the specified number of epochs\n",
    "trainer.run(train_dataloader, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model, show_shapes=False, to_file=os.path.join(working_dir, \"model_architecture.png\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39048198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define model parameters\n",
    "# BATCH_SIZE = 32\n",
    "INPUT_SHAPE = (TILE_SIZE, TILE_SIZE, band_count)  # X.shape[-1])\n",
    "\n",
    "# compile model\n",
    "\n",
    "# IF BINARY SEG, OUTPUT_CHANNELS = 1\n",
    "# model = build_unet(input_shape=INPUT_SHAPE, output_channels=1, base_filters=32, depth=6)\n",
    "model = Net(INPUT_SHAPE, output_channels=1, filters=16, depth=3).build()\n",
    "print(f\"{time.ctime()}: Model compiled successfully.\")\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    # \"accuracy\",\n",
    "    # tf.keras.metrics.Accuracy(),\n",
    "    tf.keras.metrics.BinaryAccuracy(),\n",
    "    tf.keras.metrics.BinaryIoU(),\n",
    "    # dice_coefficient,\n",
    "    # tf.keras.metrics.Precision(),\n",
    "    # tf.keras.metrics.Recall(),\n",
    "    # tfa.metrics.F1Score(num_classes=1, average='macro', threshold=0.5),\n",
    "    # tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2, threshold=0.5),\n",
    "    tf.keras.losses.BinaryCrossentropy(),\n",
    "]\n",
    "\n",
    "# tf.keras.metrics.BinaryIoU(), tf.keras.metrics.FalseNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.TruePositives()]\n",
    "\n",
    "# loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "class_weights = list(CLASS_WEIGHTS.values())\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574400af",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the learning rate scheduler\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 100, 0.96)\n",
    "    ),\n",
    "    # loss=tf.keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "# todo add widget for setting output directory for model\n",
    "output_dir = os.path.join(working_dir, \"models\")\n",
    "file_name = f\"model_{datetime.now()}\"\n",
    "file_name = file_name.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
    "model_path = os.path.join(output_dir, file_name + \".hdf5\")\n",
    "\n",
    "print(f\"{time.ctime()}: Model will be saved to {model_path}\")\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# set up callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor=\"val_binary_crossentropy\",\n",
    "    mode=\"auto\",\n",
    "    save_best_only=True,\n",
    "    restore_best_weights=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# keep track of the model training progression\n",
    "h = tf.keras.callbacks.History()\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint,\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_binary_crossentropy\", mode=\"auto\"),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=os.path.join(working_dir, \"logs\")),\n",
    "    h,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model training details\n",
    "print(f\"{time.ctime()}: Model training details:\")\n",
    "print(f\"Model name: {model.name}\")\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"Validation steps: {VALIDATION_STEPS}\")\n",
    "print(f\"Loss function: {model.loss}\")\n",
    "print(f\"Optimizer: {model.optimizer}\")\n",
    "# print(f\"Metrics: {model.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# import coiled\n",
    "# @coiled.function(\n",
    "#     cpu=8,\n",
    "#     idle_timeout=\"15s\"\n",
    "#     # package_sync_ignore=['GDAL', 'alphashape']\n",
    "# )\n",
    "def train():\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        # train_ds,  # batch size is determined by the dataset\n",
    "        epochs=25,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        # validation_data=valid_ds,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7c3a1",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03e2b9",
   "metadata": {},
   "source": [
    "## Calculate Performance Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./working/models/model_2023-07-10_20-33-44-176831.hdf5\"\n",
    "best_model = tf.keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={\n",
    "        # \"loss\": weighted_dice_loss(class_weights),\n",
    "        # \"dice_coefficient\": dice_coefficient,\n",
    "        \"BinaryCrossentropy\": tf.keras.losses.BinaryCrossentropy(),\n",
    "    },\n",
    ")  #                                        'focal_loss': semseglosses.focal_loss })\n",
    "\n",
    "# use the model to make predictions on the reserved test data\n",
    "score = best_model.evaluate(X_test, Y_test, verbose=1)\n",
    "# print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bda5c",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display n random images with their masks and predicted masks\n",
    "random_indices = np.random.choice(range(len(X_test)), size=3, replace=False)\n",
    "\n",
    "# Iterate over the randomly selected images\n",
    "for idx in random_indices:\n",
    "    # Get the original image (first three channels)\n",
    "    original_image = X_test[idx][:, :, 4]\n",
    "\n",
    "    # Get the true mask and predicted mask\n",
    "    true_mask = Y_test[idx]\n",
    "    predicted_mask = best_model.predict(X_test[idx].reshape(-1, 32, 32, 6))\n",
    "    predicted_mask = np.squeeze(predicted_mask)\n",
    "    predicted_mask = (predicted_mask > 0.2).astype(np.uint8)\n",
    "\n",
    "    # Plotting the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(5, 5))\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "\n",
    "    axs[1].imshow(true_mask, cmap=\"gray\")\n",
    "    axs[1].set_title(\"True Mask\")\n",
    "\n",
    "    axs[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = time.time()\n",
    "print(f\"{time.ctime()}:  Elapsed time: {END_TIME - START_TIME}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
