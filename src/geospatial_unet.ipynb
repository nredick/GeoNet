{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae7601f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Geospatial Analysis with Machine Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85955005",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Imports & Environment Settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bdc72",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports\n",
    "\n",
    "<!-- _You may see a message about regarding the use of a Tensorflow binary that is optimized with oneAPI Deep Neural Network Library (oneDNN). There is nothing wrong and it can be safely ignored._ -->\n",
    "\n",
    "Run the following cell to install and import neccessary libraries for this workflow. A temporary directory is created to store the data and model files.\n",
    "\n",
    "> Please note that if you rerun this cell, the temporary directory will be deleted and recreated. If you want to keep the data and model files, please copy them to a permanent location before rerunning this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries & create a temporary working directory in current folder\n",
    "# !pip install --quiet contextily earthpy fiona geopandas rasterio pyproj keras-spatial spectral\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import alphashape\n",
    "import contextily as cx\n",
    "import cv2\n",
    "import earthpy.spatial as es\n",
    "import fiona as fio\n",
    "import geopandas as gpd\n",
    "import imgaug as ia\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import spectral\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from imgaug import augmenters as iaa\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras_spatial import SpatialDataGenerator\n",
    "from osgeo import gdal\n",
    "from rasterio import windows\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.warp import Resampling, calculate_default_transform, reproject\n",
    "from shapely.geometry import box\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = time.time()\n",
    "\n",
    "# load functions and set global variables\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data dir for temporary/working files in the current directory\n",
    "working_dir = os.path.join(\".\", \"working\")\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "print(\n",
    "    f\"{time.ctime()}: Created a temporary working directory in current folder at {working_dir}\"\n",
    ")\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "\n",
    "# helper functions & global variables for the workflow\n",
    "PIXEL_SIZE = -1\n",
    "TILE_SIZE = -1\n",
    "\n",
    "\n",
    "def reproject_raster(in_path, out_path, to_crs):\n",
    "    # reproject raster to project crs\n",
    "    with rio.open(in_path) as src:\n",
    "        if src.crs == to_crs:\n",
    "            print(f\"{time.ctime()}: {in_path} is already in target CRS.\")\n",
    "            return in_path\n",
    "\n",
    "        src_crs = src.crs\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src_crs, to_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        kwargs = src.meta.copy()\n",
    "\n",
    "        kwargs.update(\n",
    "            {\"crs\": to_crs, \"transform\": transform, \"width\": width, \"height\": height}\n",
    "        )\n",
    "\n",
    "        with rio.open(out_path, \"w\", **kwargs) as dst:\n",
    "            for i in tqdm(range(1, src.count + 1)):\n",
    "                reproject(\n",
    "                    source=rio.band(src, i),\n",
    "                    destination=rio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=to_crs,\n",
    "                    resampling=Resampling.nearest,\n",
    "                )\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def encode(data_path, feature):\n",
    "    # create a numeric unique value for each attribute/feature in the data feature\n",
    "    # vector = data.copy()\n",
    "\n",
    "    data = gpd.read_file(data_path)\n",
    "    data.columns = map(str.lower, data.columns)\n",
    "\n",
    "    data = data.dropna(subset=[\"geometry\"])\n",
    "\n",
    "    feature = feature.lower()\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data[feature])\n",
    "\n",
    "    data[f\"{feature}_encoded\"] = le.transform(data[feature])\n",
    "    data[f\"{feature}_encoded\"] += 1\n",
    "\n",
    "    data.to_file(data_path)\n",
    "\n",
    "\n",
    "def get_windows(window_shape, image_shape):\n",
    "    win_rows, win_cols = window_shape\n",
    "    img_rows, img_cols = image_shape\n",
    "    offsets = itertools.product(\n",
    "        range(0, img_cols, win_cols), range(0, img_rows, win_rows)\n",
    "    )\n",
    "    image_window = windows.Window(col_off=0, row_off=0, width=img_cols, height=img_rows)\n",
    "\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(\n",
    "            col_off=col_off, row_off=row_off, width=win_cols, height=win_rows\n",
    "        )\n",
    "\n",
    "        yield window.intersection(image_window)\n",
    "\n",
    "\n",
    "def to_raster(\n",
    "    data_path,\n",
    "    output_path,\n",
    "    feature_id,\n",
    "    pixel_size=PIXEL_SIZE,\n",
    "    dtype=\"float64\",\n",
    "    windows_shape=(1024, 1024),\n",
    "):\n",
    "    encode(data_path, feature_id)\n",
    "\n",
    "    with fio.open(data_path) as features:\n",
    "        crs = features.crs\n",
    "        xmin, ymin, xmax, ymax = features.bounds\n",
    "        transform = rio.Affine.from_gdal(xmin, pixel_size, 0, ymax, 0, -pixel_size)\n",
    "        out_shape = (int((ymax - ymin) / pixel_size), int((xmax - xmin) / pixel_size))\n",
    "\n",
    "        with rio.open(\n",
    "            output_path,\n",
    "            \"w\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=dtype,\n",
    "            crs=crs,\n",
    "            transform=transform,\n",
    "            tiled=True,\n",
    "            options=[\"COMPRESS=LZW\"],\n",
    "        ) as raster:\n",
    "            for window in get_windows(windows_shape, out_shape):\n",
    "                window_transform = windows.transform(window, transform)\n",
    "                # can be smaller than windows_shape at the edges\n",
    "                window_shape = (window.height, window.width)\n",
    "                window_data = np.zeros(window_shape)\n",
    "\n",
    "                for feature in features:\n",
    "                    value = feature[\"properties\"][f\"{feature_id}_encoded\"]\n",
    "                    geom = feature[\"geometry\"]\n",
    "                    d = rasterize(\n",
    "                        [(geom, value)],\n",
    "                        all_touched=False,\n",
    "                        out_shape=window_shape,\n",
    "                        transform=window_transform,\n",
    "                    )\n",
    "                    window_data += d  # sum values up\n",
    "\n",
    "                raster.write(window_data, window=window, indexes=1)\n",
    "\n",
    "\n",
    "# function to find the number closest to n and divisible by m\n",
    "def closestDivisibleNumber(n, m):\n",
    "    if n % m == 0:\n",
    "        return n\n",
    "\n",
    "    return n - (n % m)\n",
    "\n",
    "\n",
    "def crop_center(img, cropx, cropy):\n",
    "    threeD = False\n",
    "    try:\n",
    "        _, x, y = img.shape\n",
    "        threeD = True\n",
    "    except:\n",
    "        x, y = img.shape\n",
    "\n",
    "    startx = x // 2 - cropx // 2\n",
    "    starty = y // 2 - cropy // 2\n",
    "\n",
    "    if threeD:\n",
    "        return img[:, startx : startx + cropx, starty : starty + cropy]\n",
    "\n",
    "    return img[startx : startx + cropx, starty : starty + cropy]\n",
    "\n",
    "\n",
    "def concave_hull(dataframe):\n",
    "    \"\"\"Create a single concave hull of an input GeoPandas DataFrame\"\"\"\n",
    "    flat_list = []\n",
    "\n",
    "    # Iterate over each geometry in the DataFrame\n",
    "    for geom in dataframe[\"geometry\"]:\n",
    "        # Check if the geometry is a MultiPolygon\n",
    "        if geom.geom_type == \"MultiPolygon\":\n",
    "            # Iterate over each polygon within the MultiPolygon\n",
    "            for polygon in geom.geoms:\n",
    "                # Extract the exterior coordinates of the polygon\n",
    "                flat_list.extend(list(polygon.exterior.coords))\n",
    "        else:\n",
    "            # Extract the exterior coordinates of the geometry\n",
    "            flat_list.extend(list(geom.exterior.coords))\n",
    "\n",
    "    # Create the concave hull\n",
    "    vertices = [(x, y) for x, y in flat_list]\n",
    "    # alpha = alphashape.optimizealpha(vertices) / 2\n",
    "    hull = alphashape.alphashape(vertices, 0.001)\n",
    "\n",
    "    # Create a GeoDataFrame with the concave hull\n",
    "    result = gpd.GeoDataFrame(geometry=[hull], crs=dataframe.crs)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0cfc2",
   "metadata": {},
   "source": [
    "## Setting a random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03380f96",
   "metadata": {},
   "source": [
    "### What is a random seed?\n",
    "\n",
    "A random seed is a number that is used to initialize a pseudorandom number generator. This is used to generate a sequence of numbers that are seemingly random, but are actually deterministic. This is useful for reproducibility, as the same seed will always generate the same sequence of \"random\" numbers.\n",
    "\n",
    "In short, this allows the results of this workflow to be reproducible.\n",
    "\n",
    "Set the random seed to any integer value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ed913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random seed for reproducibility\n",
    "seed_picker = widgets.IntText(value=42, description=\"Seed:\", disabled=False)\n",
    "\n",
    "seed_picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "SEED = seed_picker.value\n",
    "\n",
    "# set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(SEED)\n",
    "\n",
    "# set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# configure a new global `tensorflow` session\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1\n",
    ")\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031f898",
   "metadata": {},
   "source": [
    "# Add Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e1f81",
   "metadata": {},
   "source": [
    "## Upload your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b4c31",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Features are the inputs the model learns in order to predict a _mask_. For example, if you want to predict the land cover of a region, a feature may be soil type.\n",
    "\n",
    "Make sure to input all of the files you want to upload at once.\n",
    "\n",
    "> If you plan to re-run this workflow with the same data, you can expand the cell and and the paths to the `value` parameter, which sets a default value. For example, `value = '...data/feature1.tif,...data/feature2.tif'`.\n",
    "\n",
    "### Mask\n",
    "\n",
    "A mask defines the output the model learns to predict. For example, if you want to predict the land cover of a region, the masks would be polygons representing the land cover types.\n",
    "\n",
    "As of right now, this workflow only supports the ability to predict a single mask.\n",
    "\n",
    "You can only upload a single **vector** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the paths to your data files\n",
    "\n",
    "add_features = widgets.Textarea(\n",
    "    value=\"../chile_data/alos_nova_friburgo_5m.tif,../chile_data/2328825_2011-08-13_RE1_3A_Analytic.tif\",\n",
    "    # value=\"../california_data/dem.tif,../california_data/tri.tif,../california_data/slope.tif,../california_data/roughness.tif\",\n",
    "    placeholder=\"File paths (separated by commas)\",\n",
    "    description=\"Features:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "add_mask = widgets.Textarea(\n",
    "    value=\"../chile_data/scars.geojson\",\n",
    "    # value='../california_data/landslide_deposits.gpkg',\n",
    "    placeholder=\"File path\",\n",
    "    description=\"Mask:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = [add_features, add_mask]\n",
    "tabs.set_title(0, \"Input Feature Paths\")\n",
    "tabs.set_title(1, \"Input Mask Path\")\n",
    "\n",
    "tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91258aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the input data files to the working directory\n",
    "mask_path = shutil.copy2(\n",
    "    add_mask.value, os.path.join(working_dir, pathlib.Path(add_mask.value).name)\n",
    ")\n",
    "print(f\"{time.ctime()}: Saved a working copy of the mask to {mask_path}.\")\n",
    "\n",
    "feature_paths = []\n",
    "for f in add_features.value.split(\",\"):\n",
    "    path = os.path.join(working_dir, pathlib.Path(f).name)\n",
    "    print(f\"{time.ctime()}: Saved a working copy of the feature to {path}.\")\n",
    "    feature_paths.append(path)\n",
    "    try:\n",
    "        shutil.copy2(f, path)\n",
    "    except:\n",
    "        pass  # ignore if same file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb5d45",
   "metadata": {},
   "source": [
    "## Set the No Data Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd64fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_nodata = widgets.IntText(value=-1, description=\"no_data:\", disabled=False)\n",
    "\n",
    "select_nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed4812",
   "metadata": {},
   "source": [
    "## Select a Coordinate Reference System (CRS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d70efd",
   "metadata": {},
   "source": [
    "Input data does not all need to have the same CRS, it will be reprojected to the CRS selected here. The CRS of your mask input will be set to the default value.\n",
    "\n",
    "This workflow can use any CRS accepted by the function [`pyproj.CRS.from_user_input()`](https://geopandas.org/en/stable/docs/user_guide/projections.html):\n",
    "\n",
    "- CRS WKT string\n",
    "- An authority string (i.e. \"EPSG:4326\")\n",
    "- An EPSG integer code (i.e. 4326)\n",
    "- A pyproj.CRS\n",
    "- An object with a to_wkt method\n",
    "- PROJ string\n",
    "- Dictionary of PROJ parameters\n",
    "- PROJ keyword arguments for parameters\n",
    "- JSON string with PROJ parameters\n",
    "\n",
    "\n",
    "For reference, some common projections and their codes:\n",
    "\n",
    "- WGS84 Latitude/Longitude: \"EPSG:4326\"\n",
    "- UTM Zones (North): \"EPSG:32633\"\n",
    "- UTM Zones (South): \"EPSG:32733\"\n",
    "\n",
    "<!-- TODO add details about what is a CRS (geographic vs projected), which to select, details about why there are limited options, etc -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b47da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a CRS\n",
    "NO_DATA = select_nodata.value\n",
    "\n",
    "# set a base CRS\n",
    "default = \"EPSG:4326\"\n",
    "\n",
    "# find the CRS from the mask file\n",
    "try:  # to open the mask file as a polygon\n",
    "    tmp = gpd.read_file(mask_path)\n",
    "    default = tmp.crs\n",
    "except:  # try to open the mask file as a raster\n",
    "    tmp = rio.open(mask_path)\n",
    "    default = tmp.crs\n",
    "\n",
    "print(f\"{time.ctime()}: Detected CRS from {mask_path} to be {default}\")\n",
    "\n",
    "select_crs = widgets.Text(\n",
    "    value=str(default).upper(), description=\"CRS:\", disabled=False\n",
    ")\n",
    "\n",
    "select_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609b4a0",
   "metadata": {},
   "source": [
    "## Reproject Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2be5f",
   "metadata": {},
   "source": [
    "If *necessary*, each of the input features and the mask will be reprojected to the CRS selected above.\n",
    "\n",
    "If the following cell fails to run, try rerunning the cell. If you are still having issues, you may be using an incompatible CRS for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data to the same CRS\n",
    "CRS = select_crs.value  # get the selected CRS from the widget\n",
    "\n",
    "min_res = -1  # set to negative 1\n",
    "max_res = np.inf  # set to infinity\n",
    "\n",
    "# reproject all data to the same CRS\n",
    "for data in [mask_path, *feature_paths]:\n",
    "    try:\n",
    "        reproject_raster(data, data, CRS)\n",
    "        # calculate the minimum resolution (worst resolution of the data)\n",
    "        min_res = max(min_res, rio.open(data).res[0])\n",
    "        # calculate the maximum resolution (best resolution of the data)\n",
    "        max_res = min(max_res, rio.open(data).res[0])\n",
    "    except:\n",
    "        gdf = gpd.read_file(data)\n",
    "        if gdf.crs == CRS:\n",
    "            print(f\"{time.ctime()}: {data} is already in target CRS.\")\n",
    "        else:\n",
    "            gdf.to_crs(CRS, inplace=True)\n",
    "            os.remove(data)\n",
    "            gdf.to_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67329c",
   "metadata": {},
   "source": [
    "## Determine the bounds of the area of interest\n",
    "\n",
    "<!-- Bounds define a rectangular area of interest.\n",
    "\n",
    "You may input the bounds manually or it will be assumed that the bounds are equivalent to the extent of the mask data. -->\n",
    "\n",
    "<!-- ### Upload a file describing the bound -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e31a35",
   "metadata": {},
   "source": [
    "The bounds of the area of interest can be determined in three ways:\n",
    "\n",
    "1. DEFAULT: The bounds will be determined by a **concave hull polygon** of the mask data.\n",
    "2. Automatic determination of the **total bounds** (rectangle that encompasses all mask polygons). For more information, please see the [documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.total_bounds.html).\n",
    "3. Upload a custom area definition in the form of a vector file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd05a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select bounds determination method\n",
    "\n",
    "options = [\"Concave Hull (default)\", \"Convex Hull\", \"Total Bounds\", \"Custom Bounds\"]\n",
    "select_bounds_method = widgets.Dropdown(\n",
    "    value=options[0], description=\"Method:\", options=options, disabled=False\n",
    ")\n",
    "\n",
    "select_bounds_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475960d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the bounds\n",
    "\n",
    "mask_polygons = gpd.read_file(mask_path)\n",
    "if select_bounds_method.value == options[0]:\n",
    "    # concave hull of the mask region\n",
    "    bounds_gpd = concave_hull(mask_polygons)\n",
    "\n",
    "    # rectangular max bounds of the mask region (for cropping)\n",
    "    total_bounds = mask_polygons.total_bounds\n",
    "    total_bounds_poly = box(*total_bounds)\n",
    "    total_bounds_gs = gpd.GeoSeries(total_bounds_poly, crs=CRS)\n",
    "\n",
    "elif select_bounds_method.value == options[1]:\n",
    "    # convex hull of the mask region\n",
    "    convex_hull = mask_polygons[mask_polygons.is_valid].unary_union.convex_hull\n",
    "    bounds_gpd = gpd.GeoDataFrame(geometry=[convex_hull], crs=CRS)\n",
    "\n",
    "    # rectangular max bounds of the mask region (for cropping)\n",
    "    total_bounds = mask_polygons.total_bounds\n",
    "    total_bounds_poly = box(*total_bounds)\n",
    "    total_bounds_gs = gpd.GeoSeries(total_bounds_poly, crs=CRS)\n",
    "\n",
    "elif select_bounds_method.value == options[2]:\n",
    "    total_bounds = mask_polygons.total_bounds\n",
    "    total_bounds_poly = box(*total_bounds)\n",
    "    total_bounds_gs = gpd.GeoSeries(total_bounds_poly, crs=CRS)\n",
    "    bounds_gpd = total_bounds_gs\n",
    "\n",
    "else:\n",
    "    # creata a widget to select custom bounds\n",
    "    upload_bounds = widgets.Text(\n",
    "        value=\"Enter path...\", description=\"CRS:\", disabled=False\n",
    "    )\n",
    "\n",
    "    # display the widget\n",
    "    display(upload_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe64c71",
   "metadata": {},
   "source": [
    "### Visualize the bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18ab2b",
   "metadata": {},
   "source": [
    "The bounds will be plotted on a basemap (if available) for reference. Specifically, Esri's National Geographic World Map will be used.\n",
    "\n",
    "- The bounds will be plotted as a blue polygon.\n",
    "- The masks will be plotted in red for reference.\n",
    "\n",
    "A GEOJSON file describing the determined bounds will be saved to the working directory. This file can be used to visualize the bounds in a GIS software such as QGIS or ArcGIS.\n",
    "\n",
    "> If your area of interest is small, the background reference map may not load--this does not affect the workflow, the basemap is only provided at this step for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the bounds\n",
    "\n",
    "# custom bounds\n",
    "if select_bounds_method.value == options[2]:\n",
    "    # read bounds from file\n",
    "    bounds_gpd = gpd.read_file(upload_bounds.value)\n",
    "\n",
    "    # reproject bounds to the CRS\n",
    "    bounds_gpd = bounds_gpd.to_crs(CRS)\n",
    "\n",
    "    # get the total bounds for cropping\n",
    "    total_bounds = bounds_gpd.total_bounds\n",
    "    total_bounds_poly = box(*total_bounds)\n",
    "\n",
    "\n",
    "# save the bounds to file\n",
    "bounds_gpd.to_file(os.path.join(working_dir, \"bounds.geojson\"))\n",
    "print(f'{time.ctime()}: Saved bounds to {os.path.join(working_dir, \"bounds.geojson\")}')\n",
    "\n",
    "# plot the bounds\n",
    "bounds_ax = bounds_gpd.boundary.plot(figsize=(8, 8))\n",
    "\n",
    "# add a title to the plot\n",
    "bounds_ax.set_title(\"Bounds\")\n",
    "\n",
    "# add axis labels\n",
    "bounds_ax.set_xlabel(\"Longitude\")\n",
    "bounds_ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# add the masks to the plot\n",
    "mask_polygons.plot(ax=bounds_ax, color=\"red\")\n",
    "\n",
    "# add a basemap to the plot\n",
    "cx.add_basemap(bounds_ax, source=cx.providers.Esri.NatGeoWorldMap, crs=CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c16c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area = bounds_gpd.area[0]\n",
    "unit = bounds_gpd.crs.axis_info[0].unit_name\n",
    "print(\n",
    "    f\"{time.ctime()}: Total area of the bounds is {total_area:.2f} in square {unit}s.\"\n",
    ")\n",
    "\n",
    "obj_area = mask_polygons.area.sum()\n",
    "print(f\"{time.ctime()}: Total area of the mask is {obj_area:.2f} square {unit}s.\")\n",
    "\n",
    "ratio = obj_area / total_area\n",
    "print(f\"{time.ctime()}: Ratio of mask to bounds is {ratio:.3f}.\")\n",
    "\n",
    "CLASS_WEIGHTS = {0: 1 - ratio, 1: ratio}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823b355",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8dca3",
   "metadata": {},
   "source": [
    "## Select tile size & data resolution\n",
    "\n",
    "<!-- ### What is a tile size?\n",
    "\n",
    "### Why are tile sizes powers of 2?\n",
    "\n",
    "### How to choose a tile size -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84aa11",
   "metadata": {},
   "source": [
    "The tile size is the size of the image patches in `tile_size * tile_size` pixels that will be extracted from the input data.\n",
    "\n",
    "The resolution is automatically determined by the input data, but can be modified.\n",
    "\n",
    "<!-- It is not recommended increase the resolution of the data -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tile size and data resolution\n",
    "select_tile_size = widgets.Dropdown(\n",
    "    options=[16, 32, 64, 128, 256, 512],\n",
    "    value=64,\n",
    "    description=\"Tile Size:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "print(\"For reference:\")\n",
    "print(\n",
    "    f\"{time.ctime()}: Detected minimum (lowest) resolution of {min_res} m from input data.\"\n",
    ")\n",
    "print(\n",
    "    f\"{time.ctime()}: Detected maximum (highest) resolution of {max_res} m from input data.\"\n",
    ")\n",
    "\n",
    "select_res = widgets.BoundedFloatText(\n",
    "    # value=0.00010,  # defaults to 10 m\n",
    "    min=max_res,\n",
    "    step=0.00001,  # 1 m\n",
    "    description=\"Resolution:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# combine the widgets into an HBox layout\n",
    "widget_layout = widgets.HBox([select_tile_size, select_res])\n",
    "\n",
    "# display the layout\n",
    "widget_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80514e",
   "metadata": {},
   "source": [
    "## Select vector file features\n",
    "\n",
    "If you uploaded any features in vector file format, you will be prompted to select which features to use. Vector files may include multiple features, which will need to be encoded as separate bands in the composite/stacked raster used to train the model. If you don't select any features, the function will default to selecting the first feature it finds in the first vector file it searches.\n",
    "\n",
    "It is recommended that you only select the most relevant features to reduce the time and space complexity of the model (how long and how much memory it takes to run). Additionally, too many inputs may cloud the model's ability to learn the relationship between the inputs and the mask.\n",
    "\n",
    "> Note that this is not relevant for raster feature files, which will be processed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55129e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to train on\n",
    "\n",
    "# set tile & pixel size from widget selections\n",
    "TILE_SIZE = select_tile_size.value\n",
    "PIXEL_SIZE = select_res.value\n",
    "\n",
    "all_col = []\n",
    "\n",
    "for path in feature_paths:\n",
    "    # print(path, feature)\n",
    "    feature = Path(os.path.basename(path)).stem\n",
    "    try:  # try to open aka check if its a vector\n",
    "        gdf = gpd.read_file(path)\n",
    "        all_col.extend([f\"{feature}: {n}\" for n in gdf.columns])\n",
    "        all_col.remove(f\"{feature}: geometry\")\n",
    "    except:\n",
    "        print(\n",
    "            f\"{time.ctime()}: Feature input {path} is already a raster file, no features need to be extracted.\"\n",
    "        )\n",
    "\n",
    "# select columns to keep\n",
    "to_keep = widgets.SelectMultiple(\n",
    "    options=all_col,\n",
    "    # value=[all_col[0]], # defaults to first feature\n",
    "    description=\"Features: \",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "if len(all_col) > 0:\n",
    "    to_keep.value = [all_col[0]]  # defaults to first feature\n",
    "    display(to_keep)\n",
    "else:\n",
    "    print(f\"{time.ctime()}: You do not need to select any input features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16086bea",
   "metadata": {},
   "source": [
    "## Rasterize data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88062978",
   "metadata": {},
   "source": [
    "In order for the model to learn from the data, the input feature data will be encoded in raster bands. Therefore, any input features in vector file format will be rasterized using the data resolution selected above.\n",
    "\n",
    "> Note: Depending on the size of your data, this step may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04705a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterize masks and selected features if necessary\n",
    "try:\n",
    "    gdf = gpd.read_file(mask_path)\n",
    "    old_path = mask_path\n",
    "    mask_path = os.path.join(working_dir, \"mask.tif\")\n",
    "    gdf[\"encoding_key\"] = 1\n",
    "    os.remove(old_path)\n",
    "    gdf.to_file(old_path)\n",
    "    to_raster(\n",
    "        old_path, mask_path, feature_id=\"encoding_key\", pixel_size=select_res.value\n",
    "    )\n",
    "except:\n",
    "    print(f\"{time.ctime()}: {mask_path} is already a raster file.\")\n",
    "\n",
    "# get the feature_ids for the features that need to be turned into bands\n",
    "keeping = [re.findall(r\"\\s(.*)\", s) for s in to_keep.value]\n",
    "keeping = list(itertools.chain.from_iterable(keeping))\n",
    "keeping = list(map(str.lower, keeping))\n",
    "\n",
    "band_paths = []\n",
    "\n",
    "# convert the features to rasters if necessary\n",
    "for feature_path in feature_paths:\n",
    "    try:\n",
    "        gdf = gpd.read_file(feature_path)\n",
    "        gdf.columns = list(map(str.lower, gdf.columns))\n",
    "\n",
    "        for feature in keeping:\n",
    "            if feature in gdf.columns:\n",
    "                fn = feature + \".tif\"\n",
    "                to_raster(\n",
    "                    feature_path,\n",
    "                    os.path.join(working_dir, fn),\n",
    "                    feature_id=feature,\n",
    "                    pixel_size=PIXEL_SIZE,\n",
    "                )\n",
    "                band_paths.append(os.path.join(working_dir, fn))\n",
    "                print(\n",
    "                    f\"{time.ctime()}: {feature} is in {feature_path}, band saved as {fn}.\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"{time.ctime()}: {feature} is not in {feature_path}\")\n",
    "    except:\n",
    "        print(f\"{time.ctime()}: {feature} is already a raster file.\")\n",
    "        band_paths.append(feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924834ee",
   "metadata": {},
   "source": [
    "## Stack the features into a multiband raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfc36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all rasters are the same resolution & have x & y dims divisible by 2\n",
    "for raster in tqdm([mask_path, *band_paths]):\n",
    "    r = gdal.Open(raster)\n",
    "    gdal.Warp(\n",
    "        raster,\n",
    "        r,\n",
    "        xRes=PIXEL_SIZE,\n",
    "        yRes=PIXEL_SIZE,\n",
    "        resampleAlg=\"bilinear\",\n",
    "        multithread=True,\n",
    "        copyMetadata=True,\n",
    "        targetAlignedPixels=True,\n",
    "        outputBounds=total_bounds,\n",
    "        srcNodata=NO_DATA,\n",
    "        dstNodata=NO_DATA,\n",
    "    )\n",
    "\n",
    "# crop to bounds and save to working directory\n",
    "band_paths_list = es.crop_all(\n",
    "    [mask_path, *band_paths], working_dir, total_bounds_gs, overwrite=True\n",
    ")\n",
    "\n",
    "# build a list describing the bands to be stacked in the composite image\n",
    "band_paths = []\n",
    "mask_path = \"\"\n",
    "for path in band_paths_list:\n",
    "    if \"mask\" in path:\n",
    "        mask_path = path\n",
    "    else:\n",
    "        band_paths.append(path)\n",
    "\n",
    "# create a save path for the stacked bands in the working dir\n",
    "stack_path = os.path.join(working_dir, \"stack.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata of first band\n",
    "with rio.open(band_paths[0]) as src:\n",
    "    meta = src.meta\n",
    "\n",
    "# count the number of bands in the input layers\n",
    "band_count = 0\n",
    "for layer in band_paths:\n",
    "    with rio.open(layer, \"r\") as src:\n",
    "        band_count += src.count\n",
    "\n",
    "# update meta to reflect the number of layers\n",
    "meta.update(count=band_count)\n",
    "\n",
    "# read each layer/band and write it to stack using rasterio\n",
    "BANDS = {}\n",
    "id = 1\n",
    "with rio.open(stack_path, \"w\", **meta) as dst:\n",
    "    for layer in band_paths:\n",
    "        with rio.open(layer, \"r\") as src:\n",
    "            for band in range(1, src.count + 1):\n",
    "                l = Path(layer).stem\n",
    "                print(\n",
    "                    f\"{time.ctime()}: Writing band {band} from {l} to composite raster.\"\n",
    "                )\n",
    "                dst.write_band(id, src.read(band))\n",
    "                BANDS[f\"{os.path.basename(l)}_BAND-{band}\"] = id - 1\n",
    "                id += 1\n",
    "        src.close()\n",
    "\n",
    "print(f\"{time.ctime()}: Composite raster saved as {stack_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bca5f8",
   "metadata": {},
   "source": [
    "## Data Tiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffaee0",
   "metadata": {},
   "source": [
    "### Create tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d469550",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create a spatial data generator for the mask and data stack\n",
    "mask_sgd = SpatialDataGenerator(source=mask_path, interleave=\"pixel\")\n",
    "data_sgd = SpatialDataGenerator(source=stack_path, interleave=\"pixel\")\n",
    "\n",
    "# create regularly gridded tiles that cover the bounds of the mask w no overlap\n",
    "tile_bounds_gdf = mask_sgd.regular_grid(TILE_SIZE, TILE_SIZE, overlap=0, units=\"pixels\")\n",
    "\n",
    "# create a geodataframe of the bounds of the mask\n",
    "tiles_gdf = tile_bounds_gdf[tile_bounds_gdf.intersects(bounds_gpd.unary_union)].copy()\n",
    "\n",
    "# print the number of tiles created\n",
    "print(f\"{time.ctime()}: Created {len(tiles_gdf)} tiles size {TILE_SIZE}x{TILE_SIZE}.\")\n",
    "\n",
    "# plot the tiles\n",
    "tiles_ax = tiles_gdf.boundary.plot(figsize=(8, 8), edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "# add a title to the plot\n",
    "tiles_ax.set_title(\"Tiled Area of Interest\")\n",
    "\n",
    "# add axis labels\n",
    "tiles_ax.set_xlabel(\"Longitude\")\n",
    "tiles_ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# plot the mask polygons underneath the tiles\n",
    "mask_polygons.plot(ax=tiles_ax, color=None, edgecolor=\"red\")\n",
    "\n",
    "# plot the bounds of the area of interest\n",
    "bounds_gpd.boundary.plot(ax=tiles_ax, color=None, linewidth=2)\n",
    "\n",
    "# add a basemap to the plot\n",
    "cx.add_basemap(tiles_ax, source=cx.providers.Esri.NatGeoWorldMap, crs=CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70027e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Extract images and masks from tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceeff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data to be in the correct shape for the model\n",
    "def img_reshape(arr):\n",
    "    return arr.reshape(TILE_SIZE, TILE_SIZE, -1)\n",
    "\n",
    "\n",
    "# ensure that the data is in the correct shape for the model\n",
    "data_sgd.add_preprocess_callback(\"reshape\", img_reshape)\n",
    "mask_sgd.add_preprocess_callback(\"reshape\", img_reshape)\n",
    "\n",
    "# create generators for the input data\n",
    "X_gen = data_sgd.flow_from_dataframe(tiles_gdf, TILE_SIZE, TILE_SIZE, batch_size=1)\n",
    "Y_gen = mask_sgd.flow_from_dataframe(tiles_gdf, TILE_SIZE, TILE_SIZE, batch_size=1)\n",
    "\n",
    "# n = len(tiles_gdf) # number of tiles\n",
    "\n",
    "\n",
    "# define a function to unpack a generator\n",
    "def unpack_gen(gen):\n",
    "    stack = []  # create an empty list to store the data\n",
    "    while True:  # loop until the generator is exhausted\n",
    "        try:\n",
    "            stack.append(next(gen))  # append the next batch (1) to the list\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return np.stack(stack, axis=1)  # stack the list of batches into a single array\n",
    "\n",
    "\n",
    "X = unpack_gen(X_gen)  # unpack the data generator\n",
    "Y = unpack_gen(Y_gen)  # unpack the mask generator\n",
    "\n",
    "# reshape the data to be in the correct format for the model\n",
    "X = X[0, ...].astype(np.float32)\n",
    "Y = Y[0, ...].astype(np.float32)\n",
    "\n",
    "Y = np.where(Y > 0, 1, 0)  # binarize the mask\n",
    "\n",
    "# print(X.shape, Y.shape)\n",
    "# np.unique(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3cbd2",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Check to make sure things look good visually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1648117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three dropdowns to select the band to display\n",
    "options = list(BANDS.keys())\n",
    "options.insert(0, \"None\")\n",
    "band1 = widgets.Dropdown(options=options, description=\"Band 1\")\n",
    "band2 = widgets.Dropdown(options=options, description=\"Band 2\")\n",
    "band3 = widgets.Dropdown(options=options, description=\"Band 3\")\n",
    "\n",
    "# display the dropdowns in boxes\n",
    "b = widgets.VBox([band1, band2, band3])\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2371c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = []\n",
    "for c in b.children:\n",
    "    if c.value != \"None\":\n",
    "        bands.append(BANDS[c.value])\n",
    "\n",
    "r = 3\n",
    "if not len(bands) == 0:\n",
    "    n = np.random.randint(0, len(X), r)\n",
    "\n",
    "    # plot r random tiles in separate images\n",
    "    for i in range(r):\n",
    "        spectral.imshow(\n",
    "            X[n[i], ...],\n",
    "            bands=bands,\n",
    "            stretch=True,\n",
    "            title=f\"Tile {n[i]} with Bands {bands}\",\n",
    "            figsize=(3, 3),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 10 random images with their masks\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "for i in range(5):\n",
    "    n = np.random.randint(0, len(X))\n",
    "\n",
    "    # mask\n",
    "    ax = fig.add_subplot(1, 5, i + 1)\n",
    "    ax.imshow(Y[n, :, :, 0], cmap=\"Greys_r\")\n",
    "    ax.set_title(f\"Mask {n}\")\n",
    "\n",
    "    # image\n",
    "    ax = fig.add_subplot(2, 5, i + 1)\n",
    "    ax.imshow(X[n, :, :, 3], cmap=\"viridis\")\n",
    "    ax.set_title(f\"Image {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8c1a6",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to the correct data type\n",
    "X = np.array(X, dtype=np.float32)\n",
    "Y = np.array(Y, dtype=np.uint8)\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "    normalized_images = np.zeros_like(images, dtype=np.float32)\n",
    "    num_images, _, _, channels = images.shape\n",
    "\n",
    "    # Reshape the images to a 2D array for vectorized normalization\n",
    "    reshaped_images = images.reshape(num_images, -1)\n",
    "\n",
    "    # Perform per-channel normalization to [0, 1] range\n",
    "    for c in range(channels):\n",
    "        channel = images[:, :, :, c]\n",
    "        normalized_channel = cv2.normalize(\n",
    "            channel, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F\n",
    "        )\n",
    "        normalized_images[:, :, :, c] = normalized_channel\n",
    "\n",
    "    return normalized_images\n",
    "\n",
    "\n",
    "# normalize the images\n",
    "X_norm = normalize_images(X)\n",
    "\n",
    "print(\n",
    "    f\"{time.ctime()}: Completed per-channel, per-image normalization of {len(X_norm)} images.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979d4e",
   "metadata": {},
   "source": [
    "## Split Data & Set Parameters\n",
    "\n",
    "- add info about deciding split ratios and the other params to be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573771d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = widgets.FloatSlider(\n",
    "    value=0.65,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Training %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    # orientation=\"vertical\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "validation_size = widgets.FloatSlider(\n",
    "    value=0.25,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Validation %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    # orientation=\"vertical\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "test_size = widgets.FloatSlider(\n",
    "    value=0.10,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Test %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    # orientation=\"vertical\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "select_epochs = widgets.IntSlider(\n",
    "    value=25,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description=\"# Epochs:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\"d\",\n",
    ")\n",
    "\n",
    "# should be a factor of 2 to take advantage of the GPU resources\n",
    "select_batch_size = widgets.Dropdown(\n",
    "    options=[8, 16, 32, 64, 128],\n",
    "    value=32,\n",
    "    description=\"Batch Size:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "widgets.VBox([train_size, validation_size, test_size, select_epochs, select_batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = select_epochs.value  # default = 50\n",
    "BATCH_SIZE = select_batch_size.value  # default = 32\n",
    "\n",
    "# validate the splits\n",
    "# assert int(train_size.value + validation_size.value + test_size.value) == 1\n",
    "\n",
    "TRAIN_SIZE = train_size.value\n",
    "VALID_SIZE = validation_size.value\n",
    "TEST_SIZE = test_size.value\n",
    "\n",
    "# train test split\n",
    "print(f\"{time.ctime()}: Splitting data into train, validation, and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=TEST_SIZE, shuffle=True, random_state=SEED\n",
    ")\n",
    "\n",
    "# extricate the validation set from the training set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=VALID_SIZE, shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{time.ctime()}: Dataset sizes:\")\n",
    "print(f\"Training: {len(X_train)}\")\n",
    "print(f\"Validation: {len(X_val)}\")\n",
    "print(f\"Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49d25d",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- add info about deciding augmentation params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ea22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fliplr = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Flip L/R %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "flipud = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.00,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Flip U/D %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "transX = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Translate X %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "transY = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Translate Y %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "scaleX = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Scale X %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "scaleY = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Scale Y %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "add_noise = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Noise %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "hist_eq = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Hist. Eq. %:\",\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "box = widgets.VBox([fliplr, flipud, transX, transY, scaleX, scaleY, hist_eq])\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_images_with_mask(X, Y):\n",
    "    filtered_X = []\n",
    "    filtered_Y = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if np.any(Y[i] != 0):  # Check if any pixel in the mask is non-zero\n",
    "            filtered_X.append(X[i])\n",
    "            filtered_Y.append(Y[i])\n",
    "\n",
    "    return filtered_X, filtered_Y\n",
    "\n",
    "\n",
    "X_pos, Y_pos = filter_images_with_mask(X_train, Y_train)\n",
    "\n",
    "print(f\"{time.ctime()}: Found {len(X_pos)} images with masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment all images or just the ones with masks (objs of interest)\n",
    "augment_all = widgets.RadioButtons(\n",
    "    options=[\"Augment only images with masks\", \"Augment all images\"],\n",
    "    description=\"Augment all images or just the ones with masks?\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# choose whether to concatenate augmented or replace the original images\n",
    "concat_replace = widgets.RadioButtons(\n",
    "    options=[\"Concatenate\", \"Replace\"],\n",
    "    description=\"Replace or concatenate to original dataset?\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "widgets.VBox([augment_all, concat_replace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.seed(SEED)\n",
    "\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        iaa.Fliplr(fliplr.value),  # LR flip\n",
    "        iaa.Flipud(flipud.value),  # UD flip\n",
    "        iaa.Sometimes(\n",
    "            transX.value, iaa.TranslateX(percent=(-0.1, 0.1))\n",
    "        ),  # x translation\n",
    "        iaa.Sometimes(\n",
    "            transY.value, iaa.TranslateY(percent=(-0.1, 0.1))\n",
    "        ),  # y translation\n",
    "        iaa.Sometimes(scaleX.value, iaa.ScaleX(scale=(0.8, 1.2))),  # x scale\n",
    "        iaa.Sometimes(scaleY.value, iaa.ScaleY(scale=(0.8, 1.2))),  # y scale\n",
    "        # iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255), per_channel=True),  # add gaussian noise\n",
    "        # iaa.Sometimes(\n",
    "        #     hist_eq.value, iaa.AllChannelsHistogramEqualization()\n",
    "        # ),  # histogram equalization\n",
    "    ]\n",
    ")\n",
    "\n",
    "if augment_all.value == \"Augment all images\":\n",
    "    print(f\"{time.ctime()}: Augmenting all images...\")\n",
    "    X_aug, Y_aug = seq(images=X, segmentation_maps=Y)\n",
    "else:\n",
    "    print(f\"{time.ctime()}: Augmenting images with masks...\")\n",
    "    X_aug, Y_aug = seq(images=X_pos, segmentation_maps=Y_pos)\n",
    "\n",
    "if concat_replace.value == \"Concatenate\":\n",
    "    print(f\"{time.ctime()}: Concatenating augmented images to original dataset...\")\n",
    "    X = np.concatenate((X, X_aug), axis=0)\n",
    "    Y = np.concatenate((Y, Y_aug), axis=0)\n",
    "else:\n",
    "    print(f\"{time.ctime()}: Replacing original dataset with augmented images...\")\n",
    "    X = X_aug\n",
    "    Y = Y_aug\n",
    "\n",
    "print(\n",
    "    f\"{time.ctime()}: Augmentation completed. The dimension of the augmented training dataset is {X.shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdcd4d",
   "metadata": {},
   "source": [
    "## Convert to *tf.data.Dataset* format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the numpy arrays into tensorflow datasets\n",
    "print(f\"{time.ctime()}: Transforming numpy arrays into tensorflow datasets...\")\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "print(f\"{time.ctime()}: Batching and parallelizing the datasets...\")\n",
    "# batch the data, drop the remainder, & parallelize according to available resources\n",
    "train_ds = train_ds.batch(\n",
    "    BATCH_SIZE, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.batch(\n",
    "    BATCH_SIZE, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "test_ds = test_ds.batch(\n",
    "    BATCH_SIZE, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# use all of data points, 1 batch size at a time\n",
    "STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE\n",
    "print(f\"{time.ctime()}: Calculated steps per epoch to be: {STEPS_PER_EPOCH}\")\n",
    "\n",
    "# calculate validation steps\n",
    "VALIDATION_STEPS = len(X_val) // BATCH_SIZE\n",
    "print(f\"{time.ctime()}: Calculated validation steps to be: {VALIDATION_STEPS}\")\n",
    "\n",
    "print(f\"{time.ctime()}: Batching and parallelization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ceed3",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbad984",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build a unet\n",
    "# def bottleneck(inputs, n):\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(conv)\n",
    "\n",
    "#     return conv\n",
    "\n",
    "\n",
    "# def downsample(inputs, n):\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "#     conv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(conv)\n",
    "#     pool = MaxPooling2D((2, 2))(conv)\n",
    "#     # pool = Dropout(0.25)(pool)\n",
    "\n",
    "#     return conv, pool, n * 2\n",
    "\n",
    "\n",
    "# def upsample(inputs, residual, n):\n",
    "#     n = n // 2\n",
    "#     deconv = Conv2DTranspose(n, (3, 3), strides=(2, 2), padding=\"same\")(inputs)\n",
    "#     uconv = concatenate([deconv, residual])\n",
    "#     # uconv = Dropout(0.5)(uconv)\n",
    "#     uconv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(uconv)\n",
    "#     uconv = Conv2D(n, (3, 3), activation=\"relu\", padding=\"same\")(uconv)\n",
    "\n",
    "#     return uconv, n\n",
    "\n",
    "\n",
    "# def build_unet(input_shape, n=32):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "\n",
    "#     # downsample\n",
    "#     conv1, pool1, n = downsample(inputs, n)\n",
    "#     conv2, pooreg, n = downsample(pool1, n)\n",
    "#     conv3, pool3, n = downsample(pool2, n)\n",
    "#     conv4, pool4, n = downsample(pool3, n)\n",
    "\n",
    "#     # bottleneck\n",
    "#     conv5 = bottleneck(pool4, n)\n",
    "\n",
    "#     # upsample\n",
    "#     uconv4, n = upsample(conv5, conv4, n)\n",
    "#     uconv3, n = upsample(uconv4, conv3, n)\n",
    "#     uconv2, n = upsample(uconv3, conv2, n)\n",
    "#     uconv1, n = upsample(uconv2, conv1, n)\n",
    "\n",
    "#     outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(uconv1)\n",
    "\n",
    "#     return Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b657c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet w batch norm & dropout\n",
    "reg = tf.keras.regularizers.l1_l2()\n",
    "\n",
    "\n",
    "def double_conv_block(x, n_filters):\n",
    "    # conv2D then ReLU activation\n",
    "    x = Conv2D(\n",
    "        n_filters,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        # kernel_regularizer=reg,\n",
    "    )(x)\n",
    "    # conv2D then ReLU activation\n",
    "    x = Conv2D(\n",
    "        n_filters,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        # kernel_regularizer=reg,\n",
    "    )(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)  # feature map\n",
    "    f = BatchNormalization()(f)  # batch normalization\n",
    "    p = MaxPool2D(2)(f)  # pooled feature map\n",
    "    p = Dropout(0.3)(p)  # dropout\n",
    "\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    # upsample\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(\n",
    "        n_filters,\n",
    "        2,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        # kernel_regularizer=reg,\n",
    "    )(x)\n",
    "    # x = Conv2DTranspose(filters=n_filters, kernel_size=3, strides=(2, 2), padding=\"same\")(x)\n",
    "    # concatenate\n",
    "    x = concatenate([x, conv_features])\n",
    "    # dropout\n",
    "    x = Dropout(0.3)(x)\n",
    "    # Conv2D twice with ReLU activation\n",
    "    x = double_conv_block(x, n_filters)\n",
    "    # batch normalization\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet(input_shape, output_channels, depth=5, base_filters=64):\n",
    "    # inputs\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # encoder: contracting path - downsample\n",
    "    skips = []\n",
    "    filters = []\n",
    "    p = inputs\n",
    "    for i in range(depth):\n",
    "        nf = base_filters * 2**i\n",
    "        f, p = downsample_block(p, nf)\n",
    "        skips.append(f)\n",
    "        filters.append(nf)\n",
    "\n",
    "    # bottleneck\n",
    "    bottleneck = double_conv_block(p, base_filters * 2**depth)\n",
    "\n",
    "    # decoder: expanding path - upsample\n",
    "    u = bottleneck\n",
    "    for i in reversed(range(depth)):\n",
    "        u = upsample_block(u, skips[i], filters[i])\n",
    "\n",
    "    # check if output channels is 1 (binary) or > 1 (multiclass)\n",
    "    if output_channels == 1:\n",
    "        a_func = \"sigmoid\"\n",
    "        print(\n",
    "            f\"{time.ctime()}: Using sigmoid activation function for binary semantic segmentation.\"\n",
    "        )\n",
    "    else:\n",
    "        a_func = \"softmax\"\n",
    "        print(\n",
    "            f\"{time.ctime()}: Using softmax activation function for multiclass semantic segmentation.\"\n",
    "        )\n",
    "\n",
    "    # outputs\n",
    "    outputs = Conv2D(output_channels, 1, padding=\"same\", activation=a_func)(u)\n",
    "\n",
    "    # U-Net model with Keras Functional API\n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896adfb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Net(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        output_channels,\n",
    "        filters=32,\n",
    "        depth=3,\n",
    "        dilation_rates=[1],\n",
    "        reduction_ratio=16,\n",
    "        name=\"model\",\n",
    "    ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_channels = output_channels\n",
    "        self.filters = filters\n",
    "        self.depth = depth\n",
    "        self.dilation_rates = dilation_rates\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.name = name\n",
    "        self.skips = []\n",
    "        self.inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    def channel_attention(self, inputs):\n",
    "        # channel attention\n",
    "        channels = inputs.shape[-1]\n",
    "        avg_pool = layers.GlobalAveragePooling2D()(inputs)\n",
    "        fc1 = layers.Dense(channels // self.reduction_ratio)(avg_pool)\n",
    "        fc1 = layers.ReLU()(fc1)\n",
    "        fc2 = layers.Dense(channels)(fc1)\n",
    "        fc2 = layers.Activation(\"sigmoid\")(fc2)\n",
    "        reshaped = layers.Reshape((1, 1, channels))(fc2)\n",
    "        return layers.Multiply()([inputs, reshaped])\n",
    "\n",
    "    def spatial_attention(self, inputs, kernel_size=5):\n",
    "        # spatial attention\n",
    "        spatial_attn = layers.Conv2D(1, kernel_size, padding=\"same\")(inputs)\n",
    "        spatial_attn = layers.Activation(\"sigmoid\")(spatial_attn)\n",
    "        return layers.Multiply()([inputs, spatial_attn])\n",
    "\n",
    "    def CBAM_block(self, inputs):\n",
    "        # conv\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(inputs)\n",
    "        # bn + relu\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # channel attention\n",
    "        channel_attention_output = self.channel_attention(inputs, self.reduction_ratio)\n",
    "        # spatial attention\n",
    "        spatial_attention_output = self.spatial_attention(inputs)\n",
    "        # merge\n",
    "        output = layers.Add()([channel_attention_output, spatial_attention_output])\n",
    "        return output\n",
    "\n",
    "    def pyramid_pooling_block(self, inputs, bin_sizes=[1, 2, 3, 6]):\n",
    "        h, w, c = inputs.shape[1:]\n",
    "        pooled_outputs = []  # store pooled output tensors for each bin size\n",
    "        # remove invalid bin sizes\n",
    "        bin_sizes = [size for size in bin_sizes if min(h, w) % size == 0]\n",
    "        # print(\"bin sizes\", bin_sizes)\n",
    "        for size in bin_sizes:  # iterate over each bin size\n",
    "            x = layers.AveragePooling2D(pool_size=(h // size, w // size))(inputs)\n",
    "            x = layers.Conv2D(self.filters, 1, padding=\"same\")(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            # resize so all pooled output tensors have the same shape\n",
    "            x = layers.Reshape((h, w, -1))(x)\n",
    "            pooled_outputs.append(x)\n",
    "        # concatenate pooled output tensors\n",
    "        x = layers.Concatenate(axis=-1)(pooled_outputs)\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 1, padding=\"same\")(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        return x\n",
    "\n",
    "    # residual block with atrous convolutions\n",
    "    def residual_block(self, inputs):\n",
    "        x = inputs\n",
    "        stack = [x]\n",
    "        for rate in self.dilation_rates:\n",
    "            # conv block 1\n",
    "            x = layers.Conv2D(self.filters, 3, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # conv block 2\n",
    "            x = layers.Conv2D(self.filters, 1, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # apply channel-wise attention\n",
    "            se = self.channel_attention(x)\n",
    "            # add to stack to combine parallel atrous convolutions\n",
    "            stack.append(se)\n",
    "        # print([s.shape for s in stack])\n",
    "        x = layers.Add()(stack)\n",
    "        return x\n",
    "\n",
    "    def encoding_block(self, inputs):\n",
    "        # residual block\n",
    "        x = self.residual_block(inputs)\n",
    "        # enhance skip with spatial attention\n",
    "        skip = self.spatial_attention(x)\n",
    "        # update filters\n",
    "        self.filters *= 2\n",
    "        # downsampling with conv2d\n",
    "        x = layers.Conv2D(self.filters, 1, 2, padding=\"same\")(skip)\n",
    "        return x, skip\n",
    "\n",
    "    def decoding_block(self, inputs):\n",
    "        # calculate new number of filters\n",
    "        self.filters //= 2\n",
    "        # upsample the feature maps\n",
    "        x = layers.UpSampling2D()(inputs)\n",
    "        # concatenate skip connection from the corresponding encoding block\n",
    "        x = layers.Concatenate(axis=-1)([x, self.skips.pop()])\n",
    "        # residual block\n",
    "        stack = []\n",
    "        for rate in self.dilation_rates:\n",
    "            # conv block 1\n",
    "            x = layers.Conv2D(self.filters, 3, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # conv block 2\n",
    "            x = layers.Conv2D(self.filters, 1, padding=\"same\", dilation_rate=rate)(x)\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            # apply channel-wise attention\n",
    "            se = self.channel_attention(x)\n",
    "            # add to stack to combine parallel atrous convolutions\n",
    "            stack.append(se)\n",
    "        # print([s.shape for s in stack])\n",
    "        x = layers.Add()(stack)\n",
    "        return x\n",
    "\n",
    "    def bridge_block(self, inputs):\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(inputs)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # psp block\n",
    "        # x = self.PSPPooling_block(inputs)\n",
    "\n",
    "        # conv block\n",
    "        x = layers.Conv2D(self.filters, 3, padding=\"same\")(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # cbam block\n",
    "        # x = self.CBAM_block(x)\n",
    "\n",
    "        # residual block\n",
    "        # x = self.residual_block(x)\n",
    "        return x\n",
    "\n",
    "    def bridge_block(self, inputs):\n",
    "        x = self.pyramid_pooling_block(inputs)\n",
    "        # x = self.pyramid_pooling_block(x)\n",
    "        return x\n",
    "\n",
    "    def build(self):\n",
    "        # input block - inc to initial desired filter size\n",
    "        x = self.inputs\n",
    "        x = layers.Conv2D(self.filters, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        self.skips.append(x)\n",
    "\n",
    "        x = self.residual_block(x)\n",
    "\n",
    "        # encoding path\n",
    "        for _ in range(self.depth):\n",
    "            # print(self.filters, x.shape)\n",
    "            x, skip = self.encoding_block(x)\n",
    "            self.skips.append(skip)\n",
    "\n",
    "        # bridge\n",
    "        # print(\"bridge\", self.filters)\n",
    "        x = self.bridge_block(x)\n",
    "        # x = self.CBAM_block(x)\n",
    "\n",
    "        # decoding path\n",
    "        for _ in range(self.depth):\n",
    "            x = self.decoding_block(x)\n",
    "            # print(self.filters, x.shape)\n",
    "\n",
    "        # check if multiclass or binary classification\n",
    "        if self.output_channels > 1:\n",
    "            a_fn = \"softmax\"\n",
    "        else:\n",
    "            a_fn = \"sigmoid\"\n",
    "\n",
    "        # psp block\n",
    "\n",
    "        # get final skip connection from stack\n",
    "        x = layers.Concatenate(axis=-1)([x, self.skips.pop()])\n",
    "        # output segmentation map\n",
    "        outputs = layers.Conv2D(\n",
    "            self.output_channels, 1, padding=\"same\", activation=a_fn\n",
    "        )(x)\n",
    "        print(self.input_shape, outputs.shape)\n",
    "        return tf.keras.Model(inputs=self.inputs, outputs=outputs, name=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30517a6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# todo add widgets for hyperparameter tuning (sliders galore)\n",
    "\n",
    "# hyperparams to think about:\n",
    "\n",
    "# - train, valid, test size, tile size, pixel size\n",
    "# - epochs, batch size, optimizer, loss functions, accuracy metrics\n",
    "# - call backs, early stopping, patience\n",
    "# select loss function\n",
    "\n",
    "\n",
    "# select optimizer\n",
    "\n",
    "\n",
    "# select metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f193ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-5):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return dice_coeff\n",
    "\n",
    "\n",
    "def weighted_dice_loss(class_proportions):\n",
    "    def loss(y_true, y_pred):\n",
    "        dice = dice_coefficient(y_true, y_pred)\n",
    "\n",
    "        # compute weights based on class proportions\n",
    "        weights = tf.constant(class_proportions, dtype=tf.float32)\n",
    "\n",
    "        # index 0: background; 1: foreground\n",
    "        weighted_dice = tf.multiply(y_true * weights[1], dice) + tf.multiply(\n",
    "            (1.0 - y_true) * weights[0], dice\n",
    "        )\n",
    "        return 1.0 - tf.reduce_mean(weighted_dice)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39048198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "# BATCH_SIZE = 32\n",
    "INPUT_SHAPE = (TILE_SIZE, TILE_SIZE, band_count)  # X.shape[-1])\n",
    "\n",
    "# compile model\n",
    "\n",
    "# IF BINARY SEG, OUTPUT_CHANNELS = 1\n",
    "# model = build_model(input_shape=INPUT_SHAPE, output_channels=1, base_filters=64, depth=4)\n",
    "model = Net(INPUT_SHAPE, output_channels=1, filters=32, depth=6).build()\n",
    "print(f\"{time.ctime()}: Model compiled successfully.\")\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    \"accuracy\",\n",
    "    # tf.keras.metrics.Accuracy(),\n",
    "    # tf.keras.metrics.BinaryAccuracy(),\n",
    "    tf.keras.metrics.BinaryIoU(),\n",
    "    dice_coefficient,\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall(),\n",
    "    # tfa.metrics.F1Score(num_classes=1, average='macro', threshold=0.5),\n",
    "    # tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2, threshold=0.5),\n",
    "    tf.keras.losses.BinaryCrossentropy(),\n",
    "]\n",
    "\n",
    "# tf.keras.metrics.BinaryIoU(), tf.keras.metrics.FalseNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.TruePositives()]\n",
    "\n",
    "# loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "class_weights = list(CLASS_WEIGHTS.values())\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
    "    # loss=weighted_dice_loss(class_weights),\n",
    "    # loss_weights=class_weights,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "# free up RAM in case the model definition cells were run multiple times\n",
    "tf.keras.backend.clear_session()\n",
    "# tf.config.experimental.set_memory_growth(device='PhysicalDevice', enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model, show_shapes=True, to_file=os.path.join(working_dir, \"model_architecture.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574400af",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo add widget for setting output directory for model\n",
    "output_dir = os.path.join(working_dir, \"models\")\n",
    "file_name = f\"model_{datetime.now()}\"\n",
    "file_name = file_name.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
    "model_path = os.path.join(output_dir, file_name + \".hdf5\")\n",
    "\n",
    "print(f\"{time.ctime()}: Model will be saved to {model_path}\")\n",
    "\n",
    "# set up callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"auto\",\n",
    "    save_best_only=True,\n",
    "    restore_best_weights=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# keep track of the model training progression\n",
    "h = tf.keras.callbacks.History()\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint,\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor=\"val_loss\", mode=\"auto\"),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=os.path.join(working_dir, \"logs\")),\n",
    "    h,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model training details\n",
    "print(f\"{time.ctime()}: Model training details:\")\n",
    "print(f\"Model name: {model.name}\")\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"Validation steps: {VALIDATION_STEPS}\")\n",
    "# print(f\"Loss function: {model.loss}\")\n",
    "# print(f\"Optimizer: {model.optimizer}\")\n",
    "# print(f\"Metrics: {model.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b775a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,  # batch size is determined by the dataset\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_ds,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7c3a1",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03e2b9",
   "metadata": {},
   "source": [
    "## Calculate Performance Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./working/models/model_2023-07-10_20-33-44-176831.hdf5\"\n",
    "best_model = tf.keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={\n",
    "        \"loss\": weighted_dice_loss(class_weights),\n",
    "        \"dice_coefficient\": dice_coefficient,\n",
    "        \"BinaryCrossentropy\": tf.keras.losses.BinaryCrossentropy(),\n",
    "    },\n",
    ")  #                                        'focal_loss': semseglosses.focal_loss })\n",
    "\n",
    "# use the model to make predictions on the reserved test data\n",
    "score = best_model.evaluate(X_test, Y_test, verbose=1)\n",
    "# print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bda5c",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display n random images with their masks and predicted masks\n",
    "random_indices = np.random.choice(range(len(X_test)), size=3, replace=False)\n",
    "\n",
    "# Iterate over the randomly selected images\n",
    "for idx in random_indices:\n",
    "    # Get the original image (first three channels)\n",
    "    original_image = X_test[idx][:, :, 4]\n",
    "\n",
    "    # Get the true mask and predicted mask\n",
    "    true_mask = Y_test[idx]\n",
    "    predicted_mask = best_model.predict(X_test[idx].reshape(-1, 32, 32, 6))\n",
    "    predicted_mask = np.squeeze(predicted_mask)\n",
    "    predicted_mask = (predicted_mask > 0.2).astype(np.uint8)\n",
    "\n",
    "    # Plotting the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(5, 5))\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "\n",
    "    axs[1].imshow(true_mask, cmap=\"gray\")\n",
    "    axs[1].set_title(\"True Mask\")\n",
    "\n",
    "    axs[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = time.time()\n",
    "print(f\"{time.ctime()}:  Elapsed time: {END_TIME - START_TIME}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
